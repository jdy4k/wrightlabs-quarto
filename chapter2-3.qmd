---
title: "JC Qiime2 Pipeline"
---

This tutorial will follow a general cycle. You will first open a file from the QIIME2_pipeline folder,
edit the file, submit the file, and then look at the results. If the submission script was successful,
you will move on to the next file and continue the cycle. If it was not, then you will look at the log
file in your current directory, determine what went wrong, and then start the cycle over with the
same script.

## Import Raw Data into Qiime2

Each account contains a file called BIN600 this you should always move into this folder when
working on the cluster. Inside that folder you should find the metadata and raw data already
imported to the Cluster account and it should be ready for you to start analysis!

Before you start a quick note on notepad editors. Most all of your computers will come with a
notepad software that you can use with cyberduck to edit the scripts to submit to the cluster.
However for ease of use downloading a software called notepad ++. This notepad software
helpes colorcode items in the script to make it easier to read. While this is not required it is
highly recommended if you plan on using a notepad app to edit the scripts. If you prefer to use
the nano function this is not required.

Download link:
[https://notepad-plus-plus.org/downloads/v8.5.3/](https://notepad-plus-plus.org/downloads/v8.5.3/)

Before we can do anything with Qiime2, we need to get our data into a format it can use. That’s
what this section is for.

### 1.1_make-mapping.sh
The mapping file is a list of files with sample IDs for Qiime2 to import. So, we need to make one before analyzing our raw data.

1. Move into your `QIIME2_pipeline-updated` folder

::: {.callout-tip}
What command did you use to move into the `QIIME2_pipeline-updated` folder?
:::

2. Open your 1.1_make-mapping.sh script, either with Putty/Terminal, using `nano 1.1_make-mapping.sh` or, through Cyberduck, by right-clicking on it and choosing Edit With and then a text
editor.

3. Set the working directory to your main data folder (the folder containing the `raw_data`
folder)

``` bash
workdir = # your main data folder
```

::: {.callout-tip}
In Putty, which command can you use to print your working directory and why might this
be helpful when setting a working directory in a script?
:::

4. Submit the script to the cluster

``` bash
sbatch 1.1_make-mapping.sh
```

::: {.callout-tip}
sbatch needs to be able to find the job file. So, the above command works if you’re in the
QIIME2_pipeline folder but, you can submit from the main data folder too if you want, using `sbatch QIIME2_pipeline/1_raw_reads_import.sh`. Don’t forget the folder that you submit from will contain the `.o` log file.
:::

This should make a `mapping.csv` file in the folder you specified for “workdir”. The `mapping.csv` file should contain three columns: `sample-id`, `absolute-filepath`, and `direction`.

### 1.2_raw_reads_import.sh
This script uses the mapping file that you copied to import your data into Qiime2.

1. Move into your QIIME2_pipeline folder

2. Open your `1.2_raw_reads_import.sh` script either with Putty/Terminal, using `nano 1.2_raw_reads_import.sh` or through Cyberduck, by right-clicking on it and choosing Edit With and then a text editor.

::: {.callout-tip}
Do you prefer to use nano or a text-editor such as Notepad++ to edit scripts? If you have a
preference, why do you have this preference?
:::

3. Set the working directory to your main data folder (the location of your mapping file)

::: {.callout-tip}
For example, `workdir=/home/stenley/bin-600/`
:::

::: {.callout-warning}
Important: Your working directory is going to look different than this one because your
user directory will be named differently and the other directories you have have different
names than the directories in this path.
:::

![](images/tutorial-images-2/2-2_0.png)

For the `--type` flag, we almost always use: `SampleData[PairedEndSequencesWithQuality]`, but
refer to documentation if you are working with something different. You know you have Paired End sequences if you have R1 and R2 files, and you know if you have quality information if the extension is either `.fastq` or `.fastq.gz`. To see all available types do:

``` bash
conda activate qiime2-2022.11
### OR
qiime tools import --show-importable-types
```

::: {.callout-tip}
What outputs can you expect to see from script 1.2? What files were used as the input?
:::

5. Submit the script to the cluster

``` bash
sbatch 1.2_raw_reads_import.sh
```

`sbatch` needs to be able to find the job file. So, the above command works if you’re in the
QIIME2_pipeline folder. But, you can submit from the main data folder too if you want, using `sbatch QIIME2_pipeline/1_raw_reads_import.sh`.

::: {.callout-warning}
Don’t forget the folder that you submit from will contain the .o log file 
:::

6. Check status of job with squeue or watch squeue 

::: {.callout-warning}
This script can take a while. A `.o` file with the job number will be created once it’s finished. If you use `watch squeue`, you can use <kbd>CTRL</kbd> + <kbd>c</kbd> to exit that screen.
:::

## Quality Checking and Filtering

Before figuring out what bacterial taxa your sequences represent, you will first need to make sure that you are only submitting high quality sequences for analysis. Reads from the sequencing instrument can vary in quality, so it is important to ensure that the reads you are working with are of high quality to help guarantee good downstream results. The quality of the Illumina MiSeq platform is great; however, it is still prone to errors (less than 1%). We perform quality filtering to remove low quality sequences and to truncate sequences when they begin dropping below a specified quality score. Retention of this data could otherwise lead to erroneous conclusions from downstream analyses.

Sequences are received in the “fastq” format. This format includes both the sequences themselves, as well as the quality of each base pair in those sequences, as shown below:

![](images/tutorial-images-2/2-2_1.png)

The first line includes the unique sequence ID followed by the nucleotide sequence. The quality scores of each position are listed after the plus sign, where each character represents an ASCII-encoded quality score. These characters can also be used to calculate the quality of the sequence as a whole.

To look at what one of our `.fastq` text files looks like, choose one of the file names and use the `zcat` and head commands to view the first 10 lines in it. For instance, in the `raw_data` folder: 

``` bash
zcat Chris_1C_S181_R1_001.fastq.gz | head
```

### 2_quality_check.sh

We use this script to get quality information for the raw data.

1. Open the script using nano or Cyberduck

2. Set the working directory to your main data folder. This script needs the `paired-end-demux.qza` file produced from the last one, and that should be in your main data folder. If you don’t see the `paired-end-demux.qza` file file in Cyberduck, try hitting the refresh button towards the top of the window. You could also use ls to see if that file exists.

3. Before closing the script, note the flags and their values. In Qiime2, flags that begin with `--i` denote files used as input for the command. Likewise, `--o` flags denote flags specifying the names of output files. All filenames for viewable files generated by our pipeline begin with “VIEWABLE_”. So, if you decide to change the names of output files at any point during this tutorial or in the future, you will have to edit those flags and flags in subsequent scripts appropriately.

4. Submit the script to the cluster with `sbatch 2_quality_check.sh`. `sbatch` needs to be able to find the job file. So, the above command works if you’re in the QIIME2_pipeline folder.
5. Check status of job with `squeue` or `watch squeue`.

::: {.callout-warning}
This script can take a while. .out and .err files with the job number will be created once it’s finished. If you use watch `squeue`, you can use <kbd>Control</kbd> + <kbd>c</kbd> to exit that screen.
:::

::: {.callout-note}
What output can we expect from this script?
:::

6. Download the output `qzv` files.

7. Go to [view.qiime2.org](view.qiime2.org) and then drag and drop the downloaded VIEWABLE files into the
Drag and drop area (you have to do one at a time).

8. It’ll bring you to an Overview page (shown below), which will have information about the
number of sequences per sample. If you scroll down further than I did, you’ll see the number of sequences for each sample.

![](images/tutorial-images-2/2-2_2.png)

9. Click on the Interactive Quality Plot tab (shown in the gold rectangle above).

![](images/tutorial-images-2/2-2_3.png)

The x-axis shows the position of the base, and the y-axis shows the average Phred score at that
position.

A Phred quality score (more generally known as a “quality” or “Q” score) is a measure of
accuracy for a base in a sequence; they indicate the probability that a base call is correct.
For example, if a base has a Phred score of 20, the chance that this base call is correct is 99%.
Phred scores are calculated with a logarithm, so a Phred score of 30 indicates that the
probability of a correct base call is 99.9% for a certain position. The average Phred score of a
sequence is sometimes used to evaluate its quality. Usually, an average above 30 is considered
very good quality.

Review the two text files that are created as well. Generally, I would recommend looking at the
Hi_EE columns in each to see at what position it reaches 0.5 and 1.0 and then use the position
immediately before it. Chances are those positions will be different between the R1 and R2 files.
In script 3, you’ll see there’s different flags to specify them.

![](images/tutorial-images-2/2-2_4.png)

::: {.callout-tip}
Update 4/6/2023:
Instead of those two text files, an additional visualization
(VIEWABLE_ee_paired-end-demux.qzv) is created
The fastq_eestats tab has the information that was in the text files, with the Forward
Reads table above the Reverse Reads table.
:::

::: {.callout-note}
Based on the view of this file how would you describe the overall quality of the sequence run?
:::

### 3_dada_denoise.sh

We will filter by length and expected error.
Average expected error is the percentage of bases expected to be incorrect per 100 bases. So
by using an expected error of 1, we are allowing one base of each 100 bases to be incorrect, for
an expected error of 0.5, we are allowing one base of every 200 bases to be incorrect, and so
on.

Typically, an average expected error of 0.5 is considered very good sequence quality.

1. As with the other two scripts, open and edit it with nano or Cyberduck.

2. Save and submit the script with sbatch

Here are a list of flags for the qiime dada2 denoise-paired command:

![](images/tutorial-images-2/2-2_5.png)

A lot of the commands that we run in the tutorial up until alpha diversity essentially just have
flags for input and output, and if they do have other flags, it’s not necessary to change them.

Still, if you are ever curious about a Qiime2 command, you can see its flags by activating the
Qiime2 environment (conda activate qiime2-2022.11) and then typing the command name
followed by “--help”, e.g. qiime dada2 denoise-paired --help.

I’ll also note that whenever you see --p-n-threads, it should match ppn at the top of the script
(refer back to the Anatomy of a Script section if you have trouble finding that). The higher those
are, the faster the script will run.

However, our cluster doesn’t have infinite resources, so if you’re doing this in a workshop
setting, please do not exceed 10 ppn.

::: {.callout-note}
What files are created by this script? What files were used as the input?
:::

Let’s go through the VIEWABLE output from this script…

Note: You’ll have two sets of these, and what I’ll be showing are screenshots from different data.
All of these files are viewable by downloading them to your computer (See the “Using
Cyberduck” section if you’re having trouble) and then dragging and dropping them onto the box
at [view.qiime2.org](view.qiime2.org).

Going by the order of their creation, we have `VIEWABLE_denoising-stats.qzv` first.

![](images/tutorial-images-2/2-2_6.png)

The first column has the sample ids. The “input” column gives the number of raw sequences per
sample, so this column tells you how many sequences you started with.

All the other columns give sequence counts (or percents) after each stage of the Dada2 filtering
process. Besides the “input” column, the other ones you’ll want to note are the “non-chimeric”
columns.

Those columns give information about how much data were retained after all of the filtering
steps. The numbers in the ‘non-chimeric’ column here are equal to the total number of Amplicon
Sequence Variants (ASVs) in the sample, i.e. the total number of sequences retained. See
[here](https://www.nature.com/articles/ismej2017119) for more information about ASVs. But, for a brief
description, they are basically OTUs clustered at 100% similarity, as all sequences assigned to
the same ASV are identical. Anyway, the ‘percentage of input non-chimeric’ column has the
amount of sequences retained as a percent of the total initial number.

So, looking at the above, the topmost sample (A1) had 14,090 raw sequences, and 9,151
remained after filtering, merging, and chimera removal. Those 8,452 sequences were all
assigned to ASVs. So, 64.95% of the data were retained.

Generally, we want to retain at least half the sequences in nearly all of our samples. The CSO
data has pretty poor quality, so if you’re using that data, since this is just a learning experience,
the very large data loss is alright.

Next, we’ll look at the `VIEWABLE_unfiltered_table.qzv` file.

![](images/tutorial-images-2/2-2_7.png)

It gives information about the number of ASVs per sample (remember this is equivalent to the
values in the non-chimeric column for the previous visualization) and the frequency of ASVs.

The Overview tab provides summary statistics and histograms for both. The Table summary
section tells you how many total samples you have, the number of unique ASVs, and the total
number of ASVs in all the samples.


![](images/tutorial-images-2/2-2_8.png)

The Interactive Sample Detail tab gives the ASVs counts for each sample, with the samples
ordered by decreasing frequency. One useful function here is the Sampling Depth Slider (shown
in the dark blue rectangle).

You can see in the above picture that I have it set to a depth of 50,479. And, in the sentence
below that, Qiime2 says how many and what percentage of samples and ASVs would be
retained if I subsampled (rarefied) at that depth.

So, for the CSO data, at that depth, 302,874 ASVs and 6 samples would be retained. This
would probably not be a good depth to use for these data because most of our samples would
be excluded.

::: {.callout-note}
What sampling depth do you think would be a good one to pick for your data? Why did you pick
this?
:::

![](images/tutorial-images-2/2-2_9.png)

The Feature Detail tab lists the ASVs by decreasing total Frequency. It also says how many
samples each is found in.

Speaking of ASVs, the last visualization for this script for us to talk about is
`VIEWABLE_rep-seqs.qzv`.

![](images/tutorial-images-2/2-2_10.png)

One representative sequence from each ASV is present in this file.

Basic stats are given for these sequences. The Seven-Number Summary table shows the
proportion of sequences that are that length or shorter, with the proportion in the header and the
length in the row.

The sequences themselves are viewable in the table towards the bottom.

## Phylogenetic and Taxonomic Information

At this point, we have our ASV table, but we don’t know which bacteria are actually in our
samples or how similar the ASVs are to each other.

The scripts in this section address both of these issues.

### 4_assign_tax.sh

We use a pre-trained classifier to determine which bacteria are present in our data.

This is the first script that can make use of a metadata file.

If you don’t have a metadata file at this point, you could either make one with just a column for
sample ids, or you could just comment out this command and flags.

If you do have a metadata file, you may wish to validate it first with https://keemei.qiime2.org/. Keep in mind, 
Keemei will only tell you if there’s any formatting errors; it cannot tell you if the sample id’s in your data match the id’s 
in your metadata file

1. Open the script and edit it with the nano command or Cyberduck

2. Set the working directory to your main data folder Check that the name of the qza file with your representative sequences matches what’s
after the --i-reads flag. If it doesn’t, change what’s after the flag so the names match. Make sure the value of --p-n-jobs matches ppn at the top of the script.

3. Save and submit the script with sbatch

4. Open the .o log file associated with your job number and check that no errors occurred.

5. Check to see that the expected output files were created

6. Take a look at the VIEWABLE_taxonomy.qzv file

7. As before, download it, then drag and drop it onto Qiime2’s website.

![](images/tutorial-images-2/2-2_11.png)

You’ll see a table with three columns.

1. The first column gives the names of the ASVs

2. The second gives the assigned taxonomy

3. The third reports how confident the classifier is that it’s right about the ASVs identities

4. Next, let’s look at the VIEWABLE_taxa-bar-plots.qzv file

5. Again, download it, then drag and drop it onto Qiime2’s website.

![](images/tutorial-images-2/2-2_12.png)

As you can see, it creates a bar plot of the relative abundances, with each sample having a total
sum of 100. If you mouse over one of the bar segments, the relative abundance will be shown
for that bacteria in that sample, as seen with Betaproteobacteriales above.

You can change the taxonomic level with the drop down box under Taxonomic Level. In the
above picture, orders are shown.

::: {.callout-tip}
What level would you filter the table by if you wanted to look at the species level?
:::

The color palette is also editable, as is how the samples are sorted by. I chose to sort mine by
the “Day” column in my metadata file so samples from the same day are next to each other, with
gaps between samples from different days.

By default, the barplot is generated without referring to a metadata file, so you can sort by the
abundance of a given taxa but not by any of your metadata groupings.

::: {.callout-note}
What was the purpose of this script and how did it achieve what it was supposed to do?
:::

### 5_alpha_diversity.sh

We use this script to generate a phylogenetic tree, so this tree shows how closely related we
think our ASVs are to each other.

If you’re curious about how this script works,see the “Generate a tree for phylogenetic diversity
analyses” section [here](https://docs.qiime2.org/2019.7/tutorials/moving-pictures/).

Basically, the representative ASV sequences are aligned, in other words similarities between
sequences are determined. Those alignments are then used as the basis for determining the
phylogeny.

1. Open the script and edit it with the nano command or Cyberduck

2. Set the working directory to your main data folder. Check that the name of the qza file with your representative sequences matches what’s
after the --i-reads flag. If it doesn’t, change what’s after the flag so the names match

3. Save and submit the script with sbatch

::: {.callout-note}
What are the viewable output files in this script if there are any?
:::

## Downstream Analysis Preparation

The 6.0 script can only be one when at least one negative control is present.

The 6.1 script doesn’t yield any new information, but it does put all the necessary files into
folders for alpha and beta diversity analysis in addition to providing a few input folders for other
types of analyses.

### 6.0_decontam.sh

One of the greatest things about bacteria is they’re basically everywhere. One of the worst
things about bacteria is, they’re basically everywhere.

Contamination is often a concern when analyzing bacterial genetic data. Contamination refers to
bacteria that are in your data now but didn’t originate from your samples. So, these bacteria
could have gotten in through vectors, such as the reagents used, the air, etc.

Because of the potential for contamination, negative controls are often sequenced along with
samples.

:::{.callout-warning}
If you don’t have negative controls or your negative controls had very few sequences (<500),
you can skip this script.
:::

1. Open the script with nano or Cyberduck

2. Change the working directory to your main data folder

3. Specify the name of your .qza table
- input=

4. Specify the name of the metadata file
- meta=

5. Specify the column in the metadata file that says whether a sample is a negative or not
- column=

6. Specify the value in that column for samples that are negatives
- value=

7. Specify the name of the final decontaminated qza table
- output=

8. Specify the name of the table containing the list of ASVs filtered out as contaminants
- contam=

9. Save and submit the script
- Use the output qza table (decontam-table.qza by default) with script 6.0

::: {.callout-note}
What is the purpose of running this script? Do you always use this script? When do you use it
and when do you not?
:::

### Filtering Guidelines

The next script (6.1) is used for filtering our data based on number of sequences, but before we
do that, it’d be good to talk about some general guidelines for filtering.

We typically filter out samples that contain less than 1,000 sequences because it’s unlikely that
the sequences present are accurate representations of the communities they originated from.

1. View the VIEWABLE_unfiltered_table.qzv file again using view.qiime2.org
● If script 6.0 was run, refer to the summarized-$output text file instead (where output is
whatever you specified that variable as in script 6.0)

2. Click on the Interactive Sample Detail tab (shown below in the orange rectangle)

![](images/tutorial-images-2/2-2_13.png)

3. This will show you all the samples and the number of sequences in each in descending
order

4. Look at this table and choose the minimum number of sequences you want your
samples to have

::: {.callout-tip}
I would recommend filtering out samples that have less than 1,000 sequences
:::

::: {.callout-note}
How many samples have less than 1000 sequences? What file did you look at to find this
information?
:::

### A Quick Note about Variables

6.0 and 6.1 are set up a little differently compared to the previous scripts. Instead of specifying
options next to flags in the commands themselves, you set the value of variables to what you
want them to be and these variables are used in the commands.

![](images/tutorial-images-2/2-2_14.png)

You can see in the above example: I’m filtering out all samples that have less than 1,000
sequences; my metadata file is named meta.txt; my table file is table.qza; my taxonomy file is
taxonomy.qza, and lastly, my rooted tree file is rooted-tree.qza.

The section in the purple rectangle is creating variables with those values which are then used
later in the script.

::: {.callout-note}
There are no spaces between the variable’s name, the equal sign, and the value.
:::

You can see the benefits of using variables in the below picture…

![](images/tutorial-images-2/2-2_15.png)

The tree variable is being used five times.

Chances are your tree file is also named “rooted-tree.qza”, but if it weren’t and we didn’t use
variables in this script, you would have to specify the name of your tree file five times, instead of
just once. So, using variables can save you a lot of time.

You’ll also notice that \$tree is in orange. Depending on your text editor, it might not have any
special formatting, but the dollar sign in front of “tree” is what tells the cluster to get the value of
the tree variable instead of looking for a file literally named “tree.”

As an example, for me, cd $workdir is equivalent to cd /home/see/16S_tutorial/data.
But, cd workdir would have the cluster look for a file actually named “workdir”.

We’ve actually been using variables this entire time. When you’ve specified the value for
workdir, the workdir variable was then used to change to your working directory.

Putting spaces in these lines is a very easy mistake to make, and from experience, I can say it’s
pretty hard to tell you made it if you just glance through the script before submitting.

::: {.callout-note}
In the above picture which value is the variable?
:::

### A Quick Note on Normalization

Normalization basically refers to modifying the counts for our samples to make them more
comparable to each other.

Consider a situation in which one sample has 5,000 sequences (Sample A) and another 15,000
(Sample B). We may find that Sample B has a lot more species of bacteria than Sample A, but
this could just be because Sample B had more data available, whether because PCR worked
better for it or whatever other reason.

One way to make the samples more comparable would be to randomly select 4,000 sequences
from A and 4,000 from B and then see if B still has more species than A. In fact, subsampling
like this is a pretty common normalization strategy called rarefaction. And, we will use it for our
alpha diversity analyses (don’t worry, if you don’t recognize that term, we’ll talk about it later).

Another common normalization strategy that we’ll be using is Cumulative Sum Scaling, or CSS
normalization. The CSS technique corrects bias in the assessment of differential abundance of
our data by dividing raw counts by the cumulative sum of counts up to a certain percentile
determined by our data. We use data normalized with CSS for beta diversity analyses, while we
use rarefactions to normalize data for alpha diversity.

### 6.1_format_data.sh

Now, that you have a bit more background information about filtering and variables, we are
ready to start on script 6.1.

In addition to filtering out low abundance samples and Mitochondria and Chloroplasts, this script
exports the rooted tree, table, and taxonomy qza files.

It also creates folders for use with alpha and beta diversity analysis, biomarker analysis, and R.

1. Open the script with the nano command or Cyberduck

2. Change the working directory to your main data folder

3. Choose the minimum number of sequences a sample needs to be retained
If you want to keep all of your sequences, use 0
    - cutoff=

4. Specify the names of your metadata, table, taxonomy, and tree files
    - meta=
    - table=
    - taxonomy=
    - tree=
    - IF YOU RAN DECONTAM (6.0), table SHOULD BE “decontam-table.qza”
      - Without quotation marks

5. Save script and then submit it with sbatch

6. Check the .o and .err log files

7. Check that the alpha_div, ancom, beta_div, exported-feature-table, and R folders were
created
    - alpha_div folder should contain your metadata file, rooted-tree.qza file,
filtered_feature-table.qza, and a VIEWABLE file for that table.
    - ancom folder should contain your metadata file and filtered_feature-table.qza.
    - beta_div folder should contain your metadata file, rooted-tree.qza file, and
norm_feature-table.qza.
    - R folder should contain your metadata file (like all the other folders), rooted_tree.nwk file,
unnorm_R_table-with-taxonomy.txt file, and norm_R_table-with-taxonomy.txt
      - You can do a lot with R, like random forest analysis, partial squares discriminant
analysis, etc. For starters though, I recommend looking into the Phyloseq
package.
      - You can use R on the cluster, but it’s easier to make nice plots if you use it on
your own laptop
        - To do that, you’ll want to download R and then R Studio
      - Analyses with R are, unfortunately, not included with this tutorial.
        - With that being said, feel free to contact JCS at chensjr16@juniata.edu if
there’s any specific analyses you’re interested in doing with R.
    - The exported-feature-table folder should have taxonomy.tsv, feature-table.biom,
VIEWABLE_chloroplast-mitochondria_filtered-\$table.qzv, and summarized_feature-table.txt files
      - The VIEWABLE file contains the sequence counts per sample after mitochondria
and chloroplasts are removed; the summarized_feature-table.txt contains the
sequence counts only for the samples retained after filtering with the sequence
number cutoff.
      - There should also be three additional biom files:
norm_filtered_$cutoff.feature-table.biom, norm_table-with-taxonomy.biom, and
unnorm_table-with-taxonomy.biom
        - The first file was imported for use with Qiime2’s beta diversity commands
        - The other two were exported to the R folder

::: {.callout-note}
What directories did script 6.1 create and what folders do they all contain?
:::

::: {.callout-warning}
Before moving on, check that your alpha_div, beta_div, and ancom folders have the
following files.

![](images/tutorial-images-2/2-2_16.png)
:::

## Alpha Diversity Analysis

First of all, let’s talk about what alpha diversity actually is. We’ve mentioned it a couple times,
but we haven’t actually defined it. Basically, alpha diversity just refers to within sample diversity.

So, imagine, we have two samples, A and B. A has 30 different ASVs and B has 70 different
ASVs. We could say that B has higher alpha diversity, because it has more different ASVs, and
indeed, this is actually a measure of alpha diversity, called “observed.”

However, perhaps, A has 5,000 sequences, while B has 10,000, around twice as many. So, you
can see then, at least for this metric, B might just have higher diversity because it has more
sequences. So, we can use a normalization strategy to better compare these two samples.
For instance, we could randomly pick 4,000 sequences from A and 4,000 from B. This is called
rarefying the data. So, we could re-word the previous sentence as: we could rarefy A and B at a
depth of 4,000 sequences.

We could then see how many different features are in those sub-samples. Now, of course, you
probably wouldn’t get the same exact number if you were to repeat this process, so we repeat it
a certain number of times (as defined by the --p-iterations flag) and end up averaging the
results.

What’s more, we do this at multiple depths, with the minimum being defined by --p-min-depth
and the maximum by --p-max-depth. This allows us to see how alpha diversity, as defined by
our different metrics, changes as we use more and more sequences, which is actually really
important. We can plot the alpha diversity as the y axis and number of sequences as an x axis
to show how that diversity changes with respect to depth. These plots are called rarefaction
curves.

One common criticism of rarefaction as a normalization strategy is the fact that you’re not using
a lot of your data. But, by sampling at multiple depths, we can see if the slope of the alpha
diversity metrics levels off as we approach our maximum depth. If it does, then we probably
used enough of our data to capture a good approximation of the actual alpha diversity present in
our samples. However, if the slope is still rather steep, a higher maximum should be used.

Still, this loss of data is the reason that we only use rarefaction for alpha diversity, instead of for
both alpha and beta diversity analyses.

::: {.callout-note}
In your own words describe what Alpha diversity is Give an example.
:::

### 7.1_alpha_rarefactions.sh

This script rarefies our data at multiple depths, multiple times for each depth, and creates
viewable rarefaction plots so we can see if we chose a sufficiently high depth.

You might want to look at the VIEWABLE_filtered_\$cutoff.feature-table.qzv file or the
summarized_feature-table.txt file in the alpha_div folder

Qiime2 needs a minimum and maximum depth. It will choose ten intermediary depths to sample
at as well.

Your maximum depth should be a little less than the smallest number of sequences possessed
by your samples, at least the first time you run this script.

But, you may find that a higher depth is needed to accurately reflect your samples’ alpha
diversity.

You can set your minimum depth to a tenth of that.

1. Change your working directory to your alpha_div folder

2. Check that the value of --m-metadata-file matches the name of your metadata file

3. Change the values for the other flags as desired

![](images/tutorial-images-2/2-2_17.png)

4. You can see additional alpha diversity metrics [here](https://forum.qiime2.org/t/alpha-and-beta-diversity-explanations-and-commands/2282) 
under the Alpha Diversity Analysis section.

4. Save and submit the script

5. Review the .o and .err log files to see if there were any errors

6. Open the resulting visualization file
    - Look at the --o-visualization flag again if you’re having trouble finding it
      - It should have that name and be in your alpha_div folder if the script worked

![](images/tutorial-images-2/2-2_18.png)

These are rarefaction curves. The sequencing depth is the x axis, and the alpha diversity metric
is the y axis.

Qiime2 lets you easily switch between the different metrics you used (by using the box under
Metric) and the metadata column (by using the box under Sample Metadata Column).

If you scroll down further, you’ll see a legend saying what categories the different colors
correspond to. Here is the legend for the rarefaction plot shown above.

![](images/tutorial-images-2/2-2_19.png)

::: {.callout-note}
Based on the above image, which day had the lowest alpha diversity, and roughly, what is its
value for the Observed Features alpha diversity metric?
:::

::: {.callout-note}
What value did you choose for the minimum and maximum depths for script 7.1? Describe what
the graph looks like and why it looks that way.
:::

### 7.2_alpha_format.sh

What we want to do now is average the values for the iterations. At each depth, however many
iterations you specified are performed. Every iteration generates a table with alpha diversity
values for each sample. So, if we did 20 iterations, each sample would have 20 values to
average for each metric.

But after we do that, we need to get those averages into Qiime2. And, this script does that.

1. Open up the script

2. Change the working directory to your alpha_div folder

3. Specify the maximum rarefaction depth (max= from last script)
    - max=

4. Check that the value of --input-path for the qiime tools export command matches the
name of the visualization file for the alpha rarefactions plot

5. Save and submit the script

6. Check the .o log file for errors and check the q2_alpha folder, within the alpha_div folder,
for one .qza file for each metric
