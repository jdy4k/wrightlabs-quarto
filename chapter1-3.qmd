---
title: "Cluster Basics"
---

Purchased via an HHMI grant, the Juniata College Cluster is vital to
analyzing genetic data. We have since purchased another, which was made
possible by a donation from Wright Labs. Anyway, this section describe
how this miniature supercomputer is structured so that we can use it
effectively for analysis. The Cluster is fundamentally different from
the laptops and desktops we frequently work with, not just because it
runs Linux, but because it is composed of several powerful computers
Clustered together. 

Each powerful computer of the Cluster is called a “node." The head node
is at the top. You use this node to access your data and programs. The
worker nodes are underneath the head node; they complete jobs assigned
to them.

Users interact with the Cluster as depicted below:

<div class="code-explanation">
 - Using a laptop or desktop computer, you start an ssh session with the head node of the Cluster as you previously did.

 - Off the head node, you can get your files in order, run small programs, and prepare a job script for larger programs. However, larger programs will need to be submitted to worker nodes in order to prevent the Cluster from crashing.

 - To run large programs, the user will need to write a submission script (you will be using these later in the QIIME tutorial). Once your script is written on the head node, you will submit the job to a worker node which will process your request, sometimes taking minutes, hours, or weeks, and then deposit any final documents into your directory on the head node.

 - This allows many users to use the Cluster at the same time without fighting for resources. 
</div>

## Using Cyberduck

::: {.column-margin}
![Click to enlarge](/images/tutorial-images/Cyberduck3.png)
:::

::: {.column-margin}
![Click to enlarge](/images/tutorial-images/Cyberduck4.png)
:::

<div class="code-explanation">

 - To upload files to Cyberduck, simply drag and drop them to where you want them to be uploaded to, as shown below:

 - To edit a file with Cyberduck, right click on it and choose the program you want to use to edit it with.

 - To download a file, right click on it and choose "Download As,"

</div>

::: {.column-margin}
![Click to enlarge](/images/tutorial-images/Cyberduck5.png)
:::


## Running jobs

Everything you can do on a normal Linux system, you can also do on the Cluster. But, you will want to run all of your programs off the worker nodes rather than the head node. You connect to the head node with ssh and submit jobs to the worker nodes through the command line. It is dangerous to execute programs on the head node since it may cause the supercomputer to crash. To prevent this, we run jobs on the worker nodes, which can easily be done in three steps:

<div class="code-explanation">

 1. Create a `job.sh` script containing the commands to run our program
    (giftwrap the script)
 
 2. Submit the script using `sbatch` (give the script to the worker
    node)
 
 3. Check on the status of our job using `squeue` (see if the worker node
    is actively running the commands)
 
    - You can use `watch squeue` to see your job's status be continually
      updated
 
    - To exit that, use <kbd>CTRL</kbd> + <kbd>c</kbd>

</div>

Throughout this tutorial you will see how to submit scripts to a worker node using `sbatch` and how to check the status of running scripts using watch `squeue`. It is important to remember to check the output locations of the programs you are running for later access.

## Anatomy of a script

All of the scripts that you will be using for this tutorial are already made. However, a brief description of their various components will hopefully help you better understand how they work and make your own, should the need arise.

![Click to enlarge](/images/tutorial-images/ScriptAnnotated.png)

<!--
``` bash {.no-copy}
#!/bin/bash # <1> <2>
# PBS -l nodes=1:ppn=6 # <2>
#PBS -N qiime2 import
#PBS -j oe

#=======================================================#
#   Imports raw fastq reads into a QIIME 2 artifact
#
#   June 29, 2018
#=======================================================#

# Set your working directory to where you have your raw data sequence
workdir=/home/see/Wright_Labs/Connors_B_11.19/
cd $workdir

source activate qiime2-2019.7

## before running script make a mapping file as described in the 
##tutorial This is important because it points to the forward 
## and reverse reads which will be imported

qiime tools import \
  --type 'SampleData[PairedEndSequenceWithQuality]' \
  --input-path co_b.mapping.csv.txt \
  --output-path co_b.paired-end-demux.qza \
  --input-format PairedEndFastqManifestPhred33
```
1. Test
2. Test 2
-->

There is a lot in the above image to unpack. Basically, the scripts that you will be using can be divided into three sections,

<div class="code-explanation">

 1. The topmost (in the black rectangle) gives general information about the script itself. 
    - Things like which language the script is written in (we use BASH) and how much resources to allocate to that job are here
 2. The next section (in the gray rectangle) is just a description of what the script does.
    - This does not impact the job itself; it’s just to give the user more information about the script.
 3. The last section (in the pink rectangle) is the meat of the script, so to speak. It tells the Cluster what you actually want it to do.
    - In the above script, we are telling the Cluster to move into the folder as specified by the workdir variable, then activate the `qiime2-2019.7` environment, and finally run the qiime tools import command with a few flags

</div>

Two symbols that you will see a lot in our scripts are `#` and `\`.

In general, `#` tells the Cluster to ignore that line and continue on with the script. These lines are referred to as comments. The exceptions to this are in the black box above. But, everywhere else, the comments, as denoted by `#`, are just skipped.

`\` tells the Cluster that the command is continued on the next line. So, the qiime tools import command above is equivalent to: 

``` bash
qiime tools import --type 'SampleData[PairedEndSequenceWithQuality]' --input-path co_b.mapping.csv.txt --output-path co_b.paired-end-demux.qza --input-format PairedEndFastqManifestPhred33
```

As you can see (kind of), that command gets pretty long, so we use the `\`’s to make it more viewable within our scripts.

For a more viewable example `ls -l ~` is equivalent to:

``` bash
ls \
-l \
~
```

:::{.callout-tip}
## What do you think would happen if we submitted the above script with a `#` before `source activate`?
:::

:::{.callout-tip}
## Where would we expect to find the output file for the above script?

<details>
<summary>Hint</summary>

What is the purpose of setting the workdir variable; how is it being used?
</details>
:::

I should also note that the text editor that I am using (Notepad++) applies formatting to make the scripts easier to understand, e.g. making commented sections green. The text editor you are using may not do that, but as long as you remember what the different characters mean, you should not have an issue understanding the scripts.

## Editing and submitting scripts

There are a couple ways to edit scripts. You can edit them through the Cluster, with Putty, the terminal, or you could edit them with Cyberduck.
To edit through the Cluster…

<div class="code-explanation">

  1. `nano [SCRIPT NAME]` when you are in the same folder as the script you want to edit
    - For example, `nano 1.2_raw_reads_import.sh`

  2. Navigate using the arrow keys

  3. Set the working directory to the location of your input (typically your data directory)

  4. Set any input/output names and desired parameters

  5. Save your changes (<kbd>CTRL</kbd> + <kbd>X</kbd>)

  6. Then press <kbd>y</kbd>

  7. Then press <kbd>ENTER</kbd>

</div>

To edit through Cyberduck…

<div class="code-explanation">

  1. Right click on the script and choose "Edit With"

  2. Then pick the program you want to use to edit it

  3. The script should now be opened

  4. Make the desired changes

  5. Go to "File" then "Save" to save the script

</div>

Regardless of how you edit it, you submit scripts using `sbatch [SCRIPT NAME]`. For instance, `sbatch 1.2_raw_reads_import.sh` would submit the `1.2_raw_reads_import.sh` script to the head node, which would then give it to a worker node to run. 

You can check the progress of scripts with `squeue`.

To have the progress be continuously updated, use watch `squeue`.
You can exit watch `squeue` by using <kbd>CTRL</kbd> + <kbd>C</kbd>

Under `S`, there will be a letter indicating the job’s status

  - `Q` - Queued (waiting to run)
  - `R` - Running
  - `C` - Completed or Canceled


Every script outputs a log file in the directory you were in when it was submitted, with the name of the job followed by `.o+Job` number
So, the above script will have a file named `Humann.65745.out` and `Humann.65745.err` when it is finished, but as indicated by the `R`, it is currently still running

These log files are helpful for troubleshooting errors. The most common errors are due to incorrectly specifying your working directory or the input file(s)

You can also cancel your jobs, with `scancel`. 

If I wanted to cancel the Humann job above, I would use `scancel 65745`.