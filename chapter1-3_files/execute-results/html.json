{
  "hash": "91cde318ad3f5590c530b455381c8416",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Cluster Basics\"\ncode-annotations: hover\n---\n\nPurchased via an HHMI grant, the Juniata College Cluster is vital to\nanalyzing genetic data. We have since purchased another, which was made\npossible by a donation from Wright Labs. Anyway, this section describe\nhow this miniature supercomputer is structured so that we can use it\neffectively for analysis. The Cluster is fundamentally different from\nthe laptops and desktops we frequently work with, not just because it\nruns Linux, but because it is composed of several powerful computers\nClustered together. \n\nEach powerful computer of the Cluster is called a “node.\" The head node\nis at the top. You use this node to access your data and programs. The\nworker nodes are underneath the head node; they complete jobs assigned\nto them.\n\nUsers interact with the Cluster as depicted below:\n```{mermaid}\n  flowchart TD\n    A[You] -->|Your Hands| B(Laptop)\n    B -->|SSH Session| C{Head Node}\n    C -->|Job| D[Worker Node 1]\n    C -->|Job| E[Worker Node 2]\n    C -->|Job| F[Worker Node 3]\n    C -->|Job| G[Worker Node 4]\n```\n\n<div class=\"code-explanation\">\n - Using a laptop or desktop computer, you start an ssh session with the head node of the Cluster as you previously did.\n\n - Off the head node, you can get your files in order, run small programs, and prepare a job script for larger programs. However, larger programs will need to be submitted to worker nodes in order to prevent the Cluster from crashing.\n\n - To run large programs, the user will need to write a submission script (you will be using these later in the QIIME tutorial). Once your script is written on the head node, you will submit the job to a worker node which will process your request, sometimes taking minutes, hours, or weeks, and then deposit any final documents into your directory on the head node.\n\n - This allows many users to use the Cluster at the same time without fighting for resources. \n</div>\n\n## Using Cyberduck\n::: {.column-margin}\n![(1) Drag and drop (Click to enlarge)](/images/tutorial-images/Cyberduck3.png)\n:::\n\n::: {.column-margin}\n![(2) Edit (Click to enlarge)](/images/tutorial-images/Cyberduck4.png)\n:::\n\n::: {.column-margin}\n![(3) Download (Click to enlarge)](/images/tutorial-images/Cyberduck5.png)\n:::\nTo upload files to Cyberduck, simply drag and drop them to where you want them to be uploaded to, as shown in the images to the right^1^. To edit a file with Cyberduck, right click on it and choose the program you want to use to edit it with^2^. If you like to download a file, right click on it and choose `Download As` ^3^.\n\n## Running jobs\n\nEverything you can do on a normal Linux system, you can also do on the Cluster. But, you will want to run all of your programs off the worker nodes rather than the head node. You connect to the head node with ssh and submit jobs to the worker nodes through the command line. It is dangerous to execute programs on the head node since it may cause the supercomputer to crash. To prevent this, we run jobs on the worker nodes, which can easily be done in three steps:\n\n<div class=\"code-explanation\">\n\n 1. Create a `job.sh` script containing the commands to run our program\n    (giftwrap the script)\n \n 2. Submit the script using `sbatch` (give the script to the worker\n    node)\n \n 3. Check on the status of our job using `squeue` (see if the worker node\n    is actively running the commands)\n \n    - You can use `watch squeue` to see your job's status be continually\n      updated\n \n    - To exit that, use <kbd>CTRL</kbd> + <kbd>c</kbd>\n\n</div>\n\nThroughout this tutorial you will see how to submit scripts to a worker node using `sbatch` and how to check the status of running scripts using watch `squeue`. It is important to remember to check the output locations of the programs you are running for later access.\n\n## Anatomy of a script\n\nAll of the scripts that you will be using for this tutorial are already made. However, a brief description of their various components will hopefully help you better understand how they work and make your own, should the need arise.\n\n<!-- ![](/images/tutorial-images/ScriptAnnotated.png) -->\n\n```{bash, code-copy=false}\n#!/bin/bash # <1>\n# PBS -l nodes=1:ppn=6 # <2>\n#PBS -N qiime2 import # <3>\n#PBS -j oe\n\n#=======================================================# # <4>\n#   Imports raw fastq reads into a QIIME 2 artifact # <4>\n# # <4>\n#   June 29, 2018 # <4>\n#=======================================================# # <4>\n\n# Set your working directory to where you have your raw data sequence\nworkdir=/home/see/Wright_Labs/Connors_B_11.19/ # <5>\ncd $workdir # <6>\n\nsource activate qiime2-2019.7 # <7>\n\n# Before running script make a mapping file as described in the \n# tutorial This is important because it points to the forward \n# and reverse reads which will be imported.\n\nqiime tools import \\ # <8>\n  --type 'SampleData[PairedEndSequenceWithQuality]' \\ # <8>\n  --input-path co_b.mapping.csv.txt \\ # <8>\n  --output-path co_b.paired-end-demux.qza \\ # <8>\n  --input-format PairedEndFastqManifestPhred33 # <8>\n```\n1. Specifies the coding language of the script\n2. Specifies the number of nodes and ppn to allocate to the job. (higher the pnn, the more resources are allocated to the job)\n3. Specifies the name of the job as seen when you do qstat or watch qstat\n4. This box just gives a brief description of what the script does. It does not impact the job itself.\n5. Specifies the value for 'workdir'\n6. Changes the working directory to the value of 'workdir'\n7. 'Soure activate' is a command that activates an environment specified in the script. In this case, it is the qiime2-2019.7 environment.\n8. Here is the main command that we are running. We run the qiime tools import command with a few flags.\n\nThere is a lot in the above image to unpack. Basically, the scripts that you will be using can be divided into three sections,\n\n<div class=\"code-explanation\">\n 1. The topmost (in the black rectangle) gives general information about the script itself. \n    - Things like which language the script is written in (we use BASH) and how much resources to allocate to that job are here\n 2. The next section (in the gray rectangle) is just a description of what the script does.\n    - This does not impact the job itself; it’s just to give the user more information about the script.\n 3. The last section (in the pink rectangle) is the meat of the script, so to speak. It tells the Cluster what you actually want it to do.\n    - In the above script, we are telling the Cluster to move into the folder as specified by the workdir variable, then activate the `qiime2-2019.7` environment, and finally run the qiime tools import command with a few flags\n</div>\n\nTwo symbols that you will see a lot in our scripts are `#` and `\\`.\n\nIn general, `#` tells the Cluster to ignore that line and continue on with the script. These lines are referred to as comments. The exceptions to this are in the black box above. But, everywhere else, the comments, as denoted by `#`, are just skipped.\n\n`\\` tells the Cluster that the command is continued on the next line. So, the qiime tools import command above is equivalent to: \n\n*Put your cursor over the code below and scroll to the right.*\n\n``` bash\nqiime tools import --type 'SampleData[PairedEndSequenceWithQuality]' --input-path co_b.mapping.csv.txt --output-path co_b.paired-end-demux.qza --input-format PairedEndFastqManifestPhred33\n```\n\nAs you can see (kind of), that command gets pretty long, so we use the `\\`’s to make it more viewable within our scripts.\n\nFor a more viewable example `ls -l ~` is equivalent to:\n\n``` bash\nls \\\n-l \\\n~\n```\n\n:::{.callout-tip}\n## What do you think would happen if we submitted the above script with a `#` before `source activate`?\n:::\n\n:::{.callout-tip}\n## Where would we expect to find the output file for the above script?\n\n<details>\n<summary>Hint</summary>\n\nWhat is the purpose of setting the workdir variable; how is it being used?\n</details>\n:::\n\nI should also note that the text editor that I am using (Notepad++) applies formatting to make the scripts easier to understand, e.g. making commented sections green. The text editor you are using may not do that, but as long as you remember what the different characters mean, you should not have an issue understanding the scripts.\n\n## Editing and submitting scripts\n\nThere are a couple ways to edit scripts. You can edit them through the Cluster, with Putty, the terminal, or you could edit them with Cyberduck.\nTo edit through the Cluster…\n\n<div class=\"code-explanation\">\n\n  1. `nano [SCRIPT NAME]` when you are in the same folder as the script you want to edit\n    - For example, `nano 1.2_raw_reads_import.sh`\n\n  2. Navigate using the arrow keys\n\n  3. Set the working directory to the location of your input (typically your data directory)\n\n  4. Set any input/output names and desired parameters\n\n  5. Save your changes (<kbd>CTRL</kbd> + <kbd>X</kbd>)\n\n  6. Then press <kbd>y</kbd>\n\n  7. Then press <kbd>ENTER</kbd>\n\n</div>\n\nTo edit through Cyberduck…\n\n<div class=\"code-explanation\">\n\n  1. Right click on the script and choose \"Edit With\"\n\n  2. Then pick the program you want to use to edit it\n\n  3. The script should now be opened\n\n  4. Make the desired changes\n\n  5. Go to \"File\" then \"Save\" to save the script\n\n</div>\n\nRegardless of how you edit it, you submit scripts using `sbatch [SCRIPT NAME]`. For instance, `sbatch 1.2_raw_reads_import.sh` would submit the `1.2_raw_reads_import.sh` script to the head node, which would then give it to a worker node to run. \n\nYou can check the progress of scripts with `squeue`.\n\nTo have the progress be continuously updated, use watch `squeue`.\nYou can exit watch `squeue` by using <kbd>CTRL</kbd> + <kbd>C</kbd>\n\nUnder `S`, there will be a letter indicating the job’s status\n\n  - `Q` - Queued (waiting to run)\n  - `R` - Running\n  - `C` - Completed or Canceled\n\n\nEvery script outputs a log file in the directory you were in when it was submitted, with the name of the job followed by `.o+Job` number\nSo, the above script will have a file named `Humann.65745.out` and `Humann.65745.err` when it is finished, but as indicated by the `R`, it is currently still running\n\nThese log files are helpful for troubleshooting errors. The most common errors are due to incorrectly specifying your working directory or the input file(s)\n\nYou can also cancel your jobs, with `scancel`. \n\nIf I wanted to cancel the Humann job above, I would use `scancel 65745`.\n\n",
    "supporting": [
      "chapter1-3_files"
    ],
    "filters": [],
    "includes": {}
  }
}