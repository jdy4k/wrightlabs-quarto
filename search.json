[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Wright Labs Tutorial Book",
    "section": "",
    "text": "Welcome to the Wright Lab Tutorials! This collection of tutorials provides step-by-step guides for bioinformatics analysis using Juniata College’s Cluster computing resources.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#available-tutorials",
    "href": "index.html#available-tutorials",
    "title": "Wright Labs Tutorial Book",
    "section": "Available Tutorials",
    "text": "Available Tutorials\n\nUsing the Cluster\nLearn how to work with Juniata College’s Cluster computing infrastructure. This tutorial covers:\n\nRemotely connecting to the Cluster from your personal computer’s terminal\nBasics of working in a Linux terminal\nHow the Cluster operates\n\n\n\n\n16s Tutorial\nLearn how to use scripts on the cluster to run 16S analysis.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "chapter1-0.html",
    "href": "chapter1-0.html",
    "title": "Using the Cluster",
    "section": "",
    "text": "Bioinformatics requires a machine much stronger than the computers we use in daily life. It is helpful to have access to a Cluster computer—such as the one housed at Juniata College—to complete our analysis. In this tutorial, you will learn how to:\n\nConnect to Juniata’s wifi when off campus\nRemotely connect to Juniata’s Cluster from your personal computer’s terminal\nBasics of working in a Linux terminal\nHow the Cluster operates",
    "crumbs": [
      "Using the Cluster"
    ]
  },
  {
    "objectID": "chapter2-1.html",
    "href": "chapter2-1.html",
    "title": "Program Installation & Acquiring Data",
    "section": "",
    "text": "We need to create two environments to use our Qiime2 pipeline, one for Qiime2 and one for biom convert. Environments are essentially just collections of programs that we will use to perform various analyses.\nSome programs are incompatible with others and thus must be installed to different environments. As an added bonus, installing programs using environments can also help avoid breaking your cluster account.",
    "crumbs": [
      "16s Tutorial",
      "Program & Data Aquisition"
    ]
  },
  {
    "objectID": "chapter2-1.html#miniconda",
    "href": "chapter2-1.html#miniconda",
    "title": "Program Installation & Acquiring Data",
    "section": "Miniconda",
    "text": "Miniconda\n(summary of what miniconda is)\n\nChecking for Miniconda Installation\nTo check if you have miniconda installed, you can run the following command:\nconda list\nIf this returns a list of environments, you have miniconda installed. If you get an error instead, follow the installation instructions for miniconda in the next section.\n\n\nInstalling Miniconda\n\n\n\n\n\n\nIf you already have miniconda installed, you can skip this section.\n\n\n\nTo download and setup miniconda, you can run these commands:\nwget https://repo.anaconda.com/miniconda/Miniconda3-py37_4.10.3-Linux-x86_64.sh --no-check-certificate\nbash Miniconda3-py37_4.10.3-Linux-x86_64.sh\nAn installer for miniconda will begin in your terminal. You can follow these steps to configure it properly:\n\nHit enter a bunch of times; many of the initial prompts will be asking you to change defaults. If you are comfortable changing these default settings, feel free to do so. Otherwise, just hit enter until the next step.\nThen accept the license agreement by typing “yes”, without the quotation marks.\nHit enter after typing that.\nHit enter again. Type “yes” again at the “Do you wish the installer to initialize Miniconda3 by running conda init?” prompt.\nAfter installing it, copy the .condarc file to your home directory\n\ncp /home/see/.condarc ~\n\nOpen your .bashrc file\n\nnano ~/.bashrc\n\nAt the bottom of this file, paste by copying and right-clicking:\n\nexport LC_ALL=\"en_US.utf8\"\nexport LANG=\"en_US.utf8\"\nYour ~/.bashrc file should look something like this:\n# Source global definitions\nif [ -f /etc/bashrc ]; then\n. /etc/bashrc\nfi\n# Uncomment the following line if you don't like systemctl's auto-paging fea$\n# export SYSTEMD_PAGER=\n# User specific aliases and functions\n\n# &gt;&gt;&gt; conda initialize &gt;&gt;&gt;\n# !! Contents within this block are managed by 'conda init' !!\n__conda_setup=\"$('/home/stewacs19/miniconda3/bin/conda' 'shell.bash' 'hook' 2&gt;/dev/null)\"\nif [ $? -eq 0 ]; then\neval \"$__conda_setup\"\nelse\nif [ -f \"/home/stewacs19/miniconda3/etc/profile.d/conda.sh\" ]; then\n. \"/home/stewacs19/miniconda3/etc/profile.d/conda.sh\"\nelse\nexport PATH=\"/home/stewacs19/miniconda3/bin:$PATH\"\nfi\nfi\nunset __conda_setup\n# &lt;&lt;&lt; conda initialize &lt;&lt;&lt;\n\nexport LC_ALL=\"en_US.utf8\"\nexport LANG=\"en_US.utf8\"\n\nSave and exit the file by clicking CTRL + O and then CTRL + X.\nReload your .bashrc file by running:\n\nsource ~/.bashrc\nConfirm that you have miniconda installed by running conda list in your terminal. If you get a list of packages, congratulations! You have installed miniconda and can move on to the next section.",
    "crumbs": [
      "16s Tutorial",
      "Program & Data Aquisition"
    ]
  },
  {
    "objectID": "chapter1-3.html",
    "href": "chapter1-3.html",
    "title": "Cluster Basics",
    "section": "",
    "text": "Purchased via an HHMI grant, the Juniata College Cluster is vital to analyzing genetic data. We have since purchased another, which was made possible by a donation from Wright Labs. Anyway, this section describe how this miniature supercomputer is structured so that we can use it effectively for analysis. The Cluster is fundamentally different from the laptops and desktops we frequently work with, not just because it runs Linux, but because it is composed of several powerful computers Clustered together.\nEach powerful computer of the Cluster is called a “node.” The head node is at the top. You use this node to access your data and programs. The worker nodes are underneath the head node; they complete jobs assigned to them.\nUsers interact with the Cluster as depicted below:\nflowchart TD\n    A[You] --&gt;|Your Hands| B(Laptop)\n    B --&gt;|SSH Session| C{Head Node}\n    C --&gt;|Job| D[Worker Node 1]\n    C --&gt;|Job| E[Worker Node 2]\n    C --&gt;|Job| F[Worker Node 3]\n    C --&gt;|Job| G[Worker Node 4]",
    "crumbs": [
      "Using the Cluster",
      "Cluster Basics"
    ]
  },
  {
    "objectID": "chapter1-3.html#using-cyberduck",
    "href": "chapter1-3.html#using-cyberduck",
    "title": "Cluster Basics",
    "section": "Using Cyberduck",
    "text": "Using Cyberduck\n\n\n\n\n\n(1) Drag and drop (Click to enlarge)\n\n\n\n\n\n\n(2) Edit (Click to enlarge)\n\n\n\n\n\n\n(3) Download (Click to enlarge)\n\n\n\n\nTo upload files to Cyberduck, simply drag and drop them to where you want them to be uploaded to, as shown in the image to the right. To edit a file with Cyberduck, right click on it and choose the program you want to use to edit it with. If you like to download a file, right click on it and choose Download As.",
    "crumbs": [
      "Using the Cluster",
      "Cluster Basics"
    ]
  },
  {
    "objectID": "chapter1-3.html#running-jobs",
    "href": "chapter1-3.html#running-jobs",
    "title": "Cluster Basics",
    "section": "Running jobs",
    "text": "Running jobs\nEverything you can do on a normal Linux system, you can also do on the Cluster. But, you will want to run all of your programs off the worker nodes rather than the head node. You connect to the head node with ssh and submit jobs to the worker nodes through the command line. It is dangerous to execute programs on the head node since it may cause the supercomputer to crash. To prevent this, we run jobs on the worker nodes, which can easily be done in three steps:\n\nCreate a job.sh script containing the commands to run our program (giftwrap the script)\nSubmit the script using sbatch (give the script to the worker node)\nCheck on the status of our job using squeue (see if the worker node is actively running the commands)\n\nYou can use watch squeue to see your job’s status be continually updated\nTo exit that, use CTRL + c\n\n\nThroughout this tutorial you will see how to submit scripts to a worker node using sbatch and how to check the status of running scripts using watch squeue. It is important to remember to check the output locations of the programs you are running for later access.",
    "crumbs": [
      "Using the Cluster",
      "Cluster Basics"
    ]
  },
  {
    "objectID": "chapter1-3.html#anatomy-of-a-script",
    "href": "chapter1-3.html#anatomy-of-a-script",
    "title": "Cluster Basics",
    "section": "Anatomy of a script",
    "text": "Anatomy of a script\nAll of the scripts that you will be using for this tutorial are already made. However, a brief description of their various components will hopefully help you better understand how they work and make your own, should the need arise.\n\n\n1#!/bin/bash\n2# PBS -l nodes=1:ppn=6\n3#PBS -N qiime2 import\n#PBS -j oe\n\n4#=======================================================#\n#   Imports raw fastq reads into a QIIME 2 artifact\n#\n#   June 29, 2018\n#=======================================================#\n\n# Set your working directory to where you have your raw data sequence\n5workdir=/home/see/Wright_Labs/Connors_B_11.19/\n6cd $workdir\n\n7source activate qiime2-2019.7\n\n# Before running script make a mapping file as described in the \n# tutorial This is important because it points to the forward \n# and reverse reads which will be imported.\n\n8qiime tools import \\\n  --type 'SampleData[PairedEndSequenceWithQuality]' \\\n  --input-path co_b.mapping.csv.txt \\\n  --output-path co_b.paired-end-demux.qza \\\n  --input-format PairedEndFastqManifestPhred33\n\n1\n\nSpecifies the coding language of the script\n\n2\n\nSpecifies the number of nodes and ppn to allocate to the job. (higher the pnn, the more resources are allocated to the job)\n\n3\n\nSpecifies the name of the job as seen when you do qstat or watch qstat\n\n4\n\nThis box just gives a brief description of what the script does. It does not impact the job itself.\n\n5\n\nSpecifies the value for ‘workdir’\n\n6\n\nChanges the working directory to the value of ‘workdir’\n\n7\n\n‘Soure activate’ is a command that activates an environment specified in the script. In this case, it is the qiime2-2019.7 environment.\n\n8\n\nHere is the main command that we are running. We run the qiime tools import command with a few flags.\n\n\n\nHover over the numbers to the right of the code above to see annotations.\n\nThere is a lot in the above image to unpack. Basically, the scripts that you will be using can be divided into three sections,\n\nThe topmost (in the black rectangle) gives general information about the script itself.\n\nThings like which language the script is written in (we use BASH) and how much resources to allocate to that job are here\n\nThe next section (in the gray rectangle) is just a description of what the script does.\n\nThis does not impact the job itself; it’s just to give the user more information about the script.\n\nThe last section (in the pink rectangle) is the meat of the script, so to speak. It tells the Cluster what you actually want it to do.\n\nIn the above script, we are telling the Cluster to move into the folder as specified by the workdir variable, then activate the qiime2-2019.7 environment, and finally run the qiime tools import command with a few flags\n\n\nTwo symbols that you will see a lot in our scripts are # and \\.\nIn general, # tells the Cluster to ignore that line and continue on with the script. These lines are referred to as comments. The exceptions to this are in the black box above. But, everywhere else, the comments, as denoted by #, are just skipped.\n\\ tells the Cluster that the command is continued on the next line. So, the qiime tools import command above is equivalent to:\nqiime tools import --type 'SampleData[PairedEndSequenceWithQuality]' --input-path co_b.mapping.csv.txt --output-path co_b.paired-end-demux.qza --input-format PairedEndFastqManifestPhred33\n\nPut your cursor over the code above and scroll to the right.\n\nAs you can see (kind of), that command gets pretty long, so we use the \\’s to make it more viewable within our scripts.\nFor a more viewable example ls -l ~ is equivalent to:\nls \\\n-l \\\n~\n\n\n\n\n\n\nWhat do you think would happen if we submitted the above script with a # before source activate?\n\n\n\n\n\n\n\n\n\n\n\n\nWhere would we expect to find the output file for the above script?\n\n\n\n\n\nHint\n\nWhat is the purpose of setting the workdir variable; how is it being used?\n\n\n\nI should also note that the text editor that I am using (Notepad++) applies formatting to make the scripts easier to understand, e.g. making commented sections green. The text editor you are using may not do that, but as long as you remember what the different characters mean, you should not have an issue understanding the scripts.",
    "crumbs": [
      "Using the Cluster",
      "Cluster Basics"
    ]
  },
  {
    "objectID": "chapter1-3.html#editing-and-submitting-scripts",
    "href": "chapter1-3.html#editing-and-submitting-scripts",
    "title": "Cluster Basics",
    "section": "Editing and submitting scripts",
    "text": "Editing and submitting scripts\nThere are a couple ways to edit scripts. You can edit them through the Cluster, with Putty, the terminal, or you could edit them with Cyberduck. To edit through the Cluster…\n\nnano [SCRIPT NAME] when you are in the same folder as the script you want to edit - For example, nano 1.2_raw_reads_import.sh\nNavigate using the arrow keys\nSet the working directory to the location of your input (typically your data directory)\nSet any input/output names and desired parameters\nSave your changes (CTRL + X)\nThen press y\nThen press ENTER\n\nTo edit through Cyberduck…\n\nRight click on the script and choose “Edit With”\nThen pick the program you want to use to edit it\nThe script should now be opened\nMake the desired changes\nGo to “File” then “Save” to save the script\n\nRegardless of how you edit it, you submit scripts using sbatch [SCRIPT NAME]. For instance, sbatch 1.2_raw_reads_import.sh would submit the 1.2_raw_reads_import.sh script to the head node, which would then give it to a worker node to run.\nYou can check the progress of scripts with squeue.\nTo have the progress be continuously updated, use watch squeue. You can exit watch squeue by using CTRL + C\nUnder S, there will be a letter indicating the job’s status\n\nQ - Queued (waiting to run)\nR - Running\nC - Completed or Canceled\n\nEvery script outputs a log file in the directory you were in when it was submitted, with the name of the job followed by .o+Job number So, the above script will have a file named Humann.65745.out and Humann.65745.err when it is finished, but as indicated by the R, it is currently still running\nThese log files are helpful for troubleshooting errors. The most common errors are due to incorrectly specifying your working directory or the input file(s)\nYou can also cancel your jobs, with scancel.\nIf I wanted to cancel the Humann job above, I would use scancel 65745.",
    "crumbs": [
      "Using the Cluster",
      "Cluster Basics"
    ]
  },
  {
    "objectID": "chapter1-2.html",
    "href": "chapter1-2.html",
    "title": "Unix Shell Basics",
    "section": "",
    "text": "After logging into a secure shell session, you will now have a command line prompt from which you can navigate a Linux system. To familiarize yourself with how to do so, the following short tutorial will introduce you to the general syntax of command line structure, as well as the function of several commonly-used commands. Most of these commands involve working with files and folders. We will dive into data analysis later.",
    "crumbs": [
      "Using the Cluster",
      "Unix Shell Basics"
    ]
  },
  {
    "objectID": "chapter1-2.html#using-ls-and-general-command-structure",
    "href": "chapter1-2.html#using-ls-and-general-command-structure",
    "title": "Unix Shell Basics",
    "section": "Using ls (and general command structure)",
    "text": "Using ls (and general command structure)\nThe command ls will list the contents of your current directory (folder). Folders and directories are the same thing. In commands “directory” is often shortened to “dir” Once you have logged in to your Cluster account, type ls now to show the contents of your home directory:\nls\nCommands such as ls have options that modify their function. These options are indicated by a dash and are termed flags. So -a may be called “the a flag.” Try running ls with this flags (shown below).\nls -a\n\n\nOutput for ls -a\n\nWhere as ls shows all normal files in your directory, ls -a will show you all the files in a directory/folder, including hidden files (these files and folders will have a period before there name, for example .config or .local)\n.config .local Documents Downloads Pictures\n\nls -l\n\n\nOutput for ls -l\n\nls -l will show you more information about those files in a long list.\ndrwxr-xr-x 6 user user 4096 Sep 26 13:31 Documents/\ndrwxr-xr-x 4 user user 4096 Sep 25 00:49 Downloads/\ndrwxr-xr-x 3 user user 4096 Sep 26 13:46 Pictures/\n\n\n\n\n\n\n\nHow would you list all files (hidden ones included) with additional information?\n\n\n\n\n\nAnswer\n\nCombining both flag in the commands ls -l -a or ls -a -l (order does not matter) will give you the desired output. Note that you can also combine these flags and run ls -la or ls -al, though you will not be able to do this with all flags you use in the Linux terminal.\ndrwxr-xr-x 6 user user 4096 Sep 26 13:31 .local/\ndrwxr-xr-x 6 user user 4096 Sep 26 13:31 .config/\ndrwxr-xr-x 6 user user 4096 Sep 26 13:31 Documents/\ndrwxr-xr-x 4 user user 4096 Sep 25 00:49 Downloads/\ndrwxr-xr-x 3 user user 4096 Sep 26 13:46 Pictures/\n\n\n\nThe ls command also has a help function. To see it, type:\nls --help\n\n\n\n\n\n\nNote\n\n\n\nThere are two dashed before help.\n\n\nFrom this help menu, you can see all the available options for the ls command. Run this, then scroll to the top of the output where you will find a line that starts with Usage: .... You should see\nls [OPTION] ... [FILE] ...\n\nls: simply the command name\nOPTION is where you put your options or flags. You can think of options as modifiers of the command. In this case, -a and -l are both flags that modify the performance of the ls command. Options are often a dash followed by a single letter, if a command has a lot of possible options, they can be two dashes followed by a word.\nFILE is the folder that you are running ls on. You can think of it as the target of ls. So, running: ls /home will list all files in /home instead of listing files in your current directory.\n[] (the brackets) show that the OPTION flags and the FILE specification are optional. Basically, you do not have to include -a or even a folder to use ls. So the command ls alone is legal even though it has no flags or files.\n. . . (the dots) show that you can list several OPTION’s or FILE’s, as in: ls -a -h - l or that you can list all flags together as in: ls -ahl. This is convenient when using a lot of options.\n\nLet’s quickly read through the options for ls to see what else we can do with it.\n\n\n\n\n\n\nWhat command would you run to sort the files in /home by their creation time?\n\n\n\n\n\nHint\n\nUse the --help flag to find the correct flag for this situation.\n\n\n\n\n\n\n\n\n\nWhat does the -h option of ls do?",
    "crumbs": [
      "Using the Cluster",
      "Unix Shell Basics"
    ]
  },
  {
    "objectID": "chapter1-2.html#using-pwd-and-cd",
    "href": "chapter1-2.html#using-pwd-and-cd",
    "title": "Unix Shell Basics",
    "section": "Using pwd and cd",
    "text": "Using pwd and cd\nWhile ls lets us view the contents of our current directory, pwd tells us where that directory is located. Typing pwd will print the working directory, whereas cd will change our directory to the location of our choosing (by default, to work within your home folder).\nBefore we start exploring the Cluster using cd and ls, let’s take a moment to describe the file structure of a Linux machine. Though you cannot see it all at once, this is how some of the directory structure looks inside the Cluster. For example, notice how user accounts are all inside of /home/.\n/  &lt;------------------------ \"root directory\"\n  bin/\n  dev/\n  home/  &lt;------------------ \"home directory\"\n    399group1/\n    299group2/\n    username/  &lt;------------ \"YOUR home directory\"\n      yourfiles/\n    sabey/\n      16S_tutorial/\n        data/\n        jc_qime_pipeline/\n    otheruseraccounts/\n  share/\n\n\n\n\n\n\nNote\n\n\n\nWhen the term “your home directory” is used in this tutorial, we are referring to the directory created according to your account username, NOT the encompassing directory named /home (see diagram above).\n\n\nTo list your current location within this structure, try typing:\npwd\n\n\n\n\n\n\nBased on the results, where are you currently located in the file directory?\n\n\n\n\n\n\nNow let’s go to the root directory, at the very top of the diagram above:\ncd /\nUse ls to see the contents of this directory, then move into the share folder, then into the apps directory within it:\ncd home/share\ncd apps\n\n\n\n\n\n\nWhat is the full path of the directory we are in now?\n\n\n\n\n\nHint\n\nUse pwd.\n\n\n\n\n\n\n\n\n\nWhat are the contents of the apps directory?\n\n\n\n\n\nHint\n\nUse ls.\n\n\n\n\n\n\n\n\n\nHow could we have moved into the apps folder with a single command (instead of three)?\n\n\n\n\n\n\nNow that we have moved all the way into /home/share/apps, let’s move back into /home/share. There are two ways to do this:\nGo ‘up one directory’ by typing:\ncd ..\nUse an absolute path by typing:\ncd /home/share\nThis will take you directly to the /share directory regardless of your position beforehand. It is called an absolute path because it starts all the way back at the root of all directories. Notice how this path starts with a forward slash (/), since that is the root of the file structure.\nExplore other directories using cd foldername to move into a folder, ls to list the files inside, and cd .. to go up and out of that directory. If you ever get lost, you can type pwd to print your current location. You can also type cd all by itself to return to your home directory.\n\n\n\n\n\n\nWhat is the location of your home folder?\n\n\n\n\n\nHint\n\nUse cd then pwd.\n\n\n\nAnother very convenient feature of Linux that is especially useful for typing long file name, is auto-completion. When you are typing the name of a file or folder, you can press TAB to autocomplete the rest.\nFor example, if the only directory within /home/ that begins with “sa” is “sabey”, typing cd /home/sa followed by pressing TAB will autocomplete the path to /home/sabey. Try this to change into your own directory.",
    "crumbs": [
      "Using the Cluster",
      "Unix Shell Basics"
    ]
  },
  {
    "objectID": "chapter1-2.html#using-mkdir-to-make-a-directory",
    "href": "chapter1-2.html#using-mkdir-to-make-a-directory",
    "title": "Unix Shell Basics",
    "section": "Using mkdir to make a directory",
    "text": "Using mkdir to make a directory\nReturn to your home folder using cd then make a new folder named /linux_practice:\nmkdir linux_practice\nThe command mkdir also has a help command. To view it, type:\nmkdir --help\n\n\n\n\n\n\nWrite down the first two lines as you did for ls. Once you have finished this, move into your newly created directory and write the command you used to do so:",
    "crumbs": [
      "Using the Cluster",
      "Unix Shell Basics"
    ]
  },
  {
    "objectID": "chapter1-2.html#using-wget-to-download-a-file",
    "href": "chapter1-2.html#using-wget-to-download-a-file",
    "title": "Unix Shell Basics",
    "section": "Using wget to download a file",
    "text": "Using wget to download a file\nThe wget command also has a help file. As with ls and mkdir, open this file and focus on the first two lines, then skim the options below. Type:\nwget --help\n\n\n\n\n\n\nOnce again, write down the first two lines. Interpret the first line in simple language; what does wget do? Write your answer in the chart on the first page of this section.\n\n\n\nYou will notice that wget has even more options than ls. Are you required to use any of them?\nWhat happends when you run the wget, and why does that make sense?\n\n\nAfter your PCR amplicons are sequenced, the sequencing core will upload the data to a website from which it can be downloaded. In a web browser, you can click on various files to download them, but in a terminal, you instead use wget. Let’s practice with some sample data:\nDownloading files through the terminal:\n\nOpen this link in a web browser.\nScroll to the bottom and find the file called simm.tar.gz.\nRight-click that link.\nChoose “copy link” or “copy link address.”\nIn your folder, type wget followed by a space, then paste the link you just copied (If you are using PuTTY, you will have to right click to paste instead of using Ctrl + V )\n\ncd linux_practice\nwget http://www.drive5.com/uchime/simm.tar.gz\n\nPress enter to run wget and download the indicated file",
    "crumbs": [
      "Using the Cluster",
      "Unix Shell Basics"
    ]
  },
  {
    "objectID": "chapter1-2.html#using-gzip-to-compress-and-decompress-files",
    "href": "chapter1-2.html#using-gzip-to-compress-and-decompress-files",
    "title": "Unix Shell Basics",
    "section": "Using gzip to compress and decompress files",
    "text": "Using gzip to compress and decompress files\nLooking at the file you just downloaded, you will notice its name ends with tar.gz, which tells us two things. First, .tar means it is a whole folder inside of a single file (called a tarball). Second, .gz means the file is compressed with gzip.\n\n\n\n\n\n\nHow large is the file you just downloaded? (Hint: use ls with these two flags: -l, which produces a long list, and -h which displays the file sizes in a human-readable format). The file sizes will appear just to the left of the file names.\n\n\n\nAlso, (again) write down the first two lines of gzip --help.\n\n\nTo decompress the file we just downloaded, run:\ngzip -d simm.tar.gz\nYou can also use the command gunzip (literally “g” + “unzip”). gzip -d and gunzip complete the same task. Remember that you can also use TAB to autocomplete file names as you type them in.\n\n\n\n\n\n\nUsing the same ls -lh command, what is the size of the file now?\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is the compression percentage of gzip on this file?\n\n\n\n\n\nHint\n\nDivide the .tar.gz file size by the .tar file size and do not forget to convert Mb to Kb, where 1 Mb = 1024 Kb\n\n\n\nFor practice using gzip, let’s recompress the file you just decompressed by using one the options from the help tab.\n\n\n\n\n\n\nWarning\n\n\n\nOmit the -d when you run this command since we are not decompressing!\n\n\n\n\n\n\n\n\nWhy might gzip be useful for large data files like the genetic data in our 16S sequences?",
    "crumbs": [
      "Using the Cluster",
      "Unix Shell Basics"
    ]
  },
  {
    "objectID": "chapter1-2.html#using-cp-to-copy-files",
    "href": "chapter1-2.html#using-cp-to-copy-files",
    "title": "Unix Shell Basics",
    "section": "Using cp to copy files",
    "text": "Using cp to copy files\nThere are three ways to use the cp command to copy files. We will focus on these two:\ncp [OPTION] ... [-T] SOURCE DESK\nor,\ncp [OPTION] ... SOURCE ... DIRECTORY\n\n\n\n\n\n\nWhich of those parameters are not optional and why does that make sense?\n\n\n\n\n\n\nCopy the file you downloaded into a new file called seqs.tar.gz:\ncp simm.tar.gz seqs.tar.gz\nMake a new directory within your /linux_practice folder called /downloads by typing:\nmkdir downloads\nThen copy seqs.tar.gz into it:\ncp seqs.tar.gz downloads\nMove to your new directory and make sure the file copied successfully:\ncd downloads\nls\nGo back up one directory by typing:\ncd ..\nThen use ls to view the files. You should see that the seqs.tar.gz file is in both locations.\n\n\n\n\n\n\nWarning\n\n\n\nCopying can overwrite files. When you choose a destination that contains a file with the same name as that you are copying, the original file will be replaced. To avoid this, use unique names for important files.",
    "crumbs": [
      "Using the Cluster",
      "Unix Shell Basics"
    ]
  },
  {
    "objectID": "chapter1-2.html#using-mv-to-move-and-rename-files",
    "href": "chapter1-2.html#using-mv-to-move-and-rename-files",
    "title": "Unix Shell Basics",
    "section": "Using mv to move and rename files",
    "text": "Using mv to move and rename files\nThe process of moving files is very similar to that for copying them The first lines of mv --help read:\nmv [OPTION]… [-T] SOURCE DEST\nor,\nmv [OPTION]… SOURCE… DIRECTORY\nMove the original file you downloaded in the downloads directory:\nmv simm.tar.gz downloads\nYou also have two copies of the seqs.tar.gz file, one in your current directory and one in the /downloads directory that you copied there previously. Use mv to move the remaining copy into the /downloads folder with:\nmv seqs.tar.gz downloads\n\n\n\n\n\n\nWhat happens when you move two files with the same name into one directory?\n\n\n\n\n\n\nJust like copying, moving can also overwrite files. The file you just moved into the /downloads directory has replaced the one you copied earlier.\nIn Linux, renaming files is the same as moving them. Go into the /downloads folder:\ncd downloads\nThen change simm.tar.gz to also_simm.fastq.gz\nmv simm.tar.gz also_simm.fastq.gz\n\n\n\n\n\n\nRun ls and write down the current contents of the /downloads directory.\n\n\n\n\n\nAnswer\n\nIt should appear as below:\nalso_simm.fatq.gz  seqs.tar.gz",
    "crumbs": [
      "Using the Cluster",
      "Unix Shell Basics"
    ]
  },
  {
    "objectID": "chapter1-2.html#using-rm-to-remove-files",
    "href": "chapter1-2.html#using-rm-to-remove-files",
    "title": "Unix Shell Basics",
    "section": "Using rm to remove files",
    "text": "Using rm to remove files\nThe command rm has the same function as deleting files in Windows or MacOs.\nrm [OPTION]… [FILE]…\n\n\n\n\n\n\nRemove one of the files in your /downloads directory and write the command you used.\n\n\n\n\n\n\nChange out of your /downloads directory and into your lastname directory:\ncd ..\nBy default, rm only removes files, not directories. This is done to ensure that the user certainly wants to remove an entire directory. So use the -r flag to recursively remove the /downloads folder and everything within it. But use caution, as once you remove something, it cannot be retrieved (i.e. there is no trash directory that store deleted files).\n\n\n\n\n\n\nKnowing this, try to remove the downloads directory and all of the files listed inside it.\n\n\n\n\n\n\nUse ls to check that /downloads has been successfully removed.",
    "crumbs": [
      "Using the Cluster",
      "Unix Shell Basics"
    ]
  },
  {
    "objectID": "chapter1-2.html#using-top-to-view-system-resources",
    "href": "chapter1-2.html#using-top-to-view-system-resources",
    "title": "Unix Shell Basics",
    "section": "Using top to view system resources",
    "text": "Using top to view system resources\nEvery program discussed so far helps you move and change files. With the top command, you can view all of the programs running on the Cluster. Try typing it now and take note of what programs are running:\ntop\nThis provides a summary of the amount of load your system is under, and a list of the processes that are using the most CPU and memory. It should look something like this,\ntop - 13:48:46 up  1:26,  1 user,  load average: 0.31, 0.26, 0.27\nTasks: 356 total, 1 running, 355 sleep, 0 d-sleep, 0 stopped, 0 zombie\n%Cpu(s):  0.2 us,  0.2 sy,  0.0 ni, 99.6 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st\nMiB Mem :  15910.9 total,   9179.9 free,   3924.1 used,   3129.3 buff/cache\nMiB Swap:   4096.0 total,   4096.0 free,      0.0 used.  11986.8 avail Mem\n\nPID     USER      PR  NI    VIRT    RES    SHR S  %CPU  %MEM     TIME+ COMMAND\n146309  user      20   0   45960  42156  10528 S   1.0   0.3   0:13.52 nvim\n2136    user      20   0    7668   3632   3156 S   0.3   0.0   0:01.58 dbus\n4798    user      20   0   33.0g 321624 203520 S   0.3   2.0   0:58.65 firefox\n9909    user      20   0   69756  66084  13024 S   0.3   0.4   1:23.23 nvim\n13966   user      20   0 1365696  64524  41540 S   0.3   0.4   0:02.32 ueberzug\n1       root      20   0   22876  13784   9748 S   0.0   0.1   0:00.84 systemd\n2       root      20   0       0      0      0 S   0.0   0.0   0:00.00 kthreadd\n3       root      20   0       0      0      0 S   0.0   0.0   0:00.00 pool\n...     ...       ...  ...     ...    ...  ... ... ...   ...   ...     ...\n\n\n\n\n\n\nHow much RAM (memory) is the Cluster currently using?\n\n\n\n\n\n\n\n\n\n\n\n\nWrite what top shows you on the first page.",
    "crumbs": [
      "Using the Cluster",
      "Unix Shell Basics"
    ]
  },
  {
    "objectID": "chapter1-1.html",
    "href": "chapter1-1.html",
    "title": "Connecting to the Cluster",
    "section": "",
    "text": "To use the Linux-based computer Cluster housed at Juniata College, you have to connect remotely to the supercomputer. This can be done through any computer running MacOS or Windows using a secure shell session, also called ssh. Many bioinformatics pipelines are built for a Linux environment because of its power and flexibility when working with large amounts of data. Linux is an operating system much like Unix, but unlike Windows and MacOS operating systems, it runs primarily through a command line interface rather than the visible desktop to which many of us are accustomed. Through this interface of the Cluster users are able to select many options and parameters in running its installed programs.",
    "crumbs": [
      "Using the Cluster",
      "Connecting to the Cluster"
    ]
  },
  {
    "objectID": "chapter1-1.html#via-ssh-on-windows",
    "href": "chapter1-1.html#via-ssh-on-windows",
    "title": "Connecting to the Cluster",
    "section": "Via SSH on Windows",
    "text": "Via SSH on Windows\nIf you have a PC running a windows operating system, you will need to download PuTTY. Upon opening PuTTY, this a window should appear.\n\n\n\n\n\nPuTTY Configuration Window (Click to enlarge)\n\n\n\nType the Cluster’s IP address 147.73.20.123 into the “Host Name” box\nEnsure that “SSH” is selected for “Connection Type.”\nPress “Open”\nType your username (then enter) and password in the terminal window.\n\nPassword: 4gcatRocks!\n\n\n\n\n\n\n\n\nWarning\n\n\n\nAs you type your password, it will not show anything at all (not even asterisks or dots as you type). There is a password there even if you cannot see it! Be brave! If the password is typed incorrectly, press enter to again reveal the password prompt and start over.",
    "crumbs": [
      "Using the Cluster",
      "Connecting to the Cluster"
    ]
  },
  {
    "objectID": "chapter1-1.html#via-ssh-on-macos",
    "href": "chapter1-1.html#via-ssh-on-macos",
    "title": "Connecting to the Cluster",
    "section": "Via SSH on MacOs",
    "text": "Via SSH on MacOs\nOpen the terminal application to bring up the command line interface that will allow you to connect to the Cluster. Type ssh your_username_here@147.73.20.123 followed by enter, then your password.\n\n\n\n\n\n\nWarning\n\n\n\nAs you type your password, it will not show anything at all (not even asterisks or dots as you type). There is a password there even if you cannot see it! Be brave! If the password is typed incorrectly, press enter to again reveal the password prompt and start over.",
    "crumbs": [
      "Using the Cluster",
      "Connecting to the Cluster"
    ]
  },
  {
    "objectID": "chapter1-1.html#via-cyberduck",
    "href": "chapter1-1.html#via-cyberduck",
    "title": "Connecting to the Cluster",
    "section": "Via Cyberduck",
    "text": "Via Cyberduck\nTo help view and edit files on the Cluster, both Mac and Windows users can install the program “Cyberduck” on their computer:\n\nDownload Cyberduck\nClick the box on the left to select the download appropriate for your operating system\nFollow the onscreen instructions to install the program to your device\nOpen the Cyberduck application\nIn Cyberduck, click “Open Connection” in the upper left hand section of the window.\n\nChange FTP (File Transfer Protocol) to SFTP (SSH File Transfer Protocol)\nIn the “Server” box type 147.73.20.123 to connect to Juniata’s new Cluster\nPut in your username and password\nClick “Connect”\n\nGo to the preferences\n\nWindows: Edit -&gt; Preferences\nMac: Cyberduck -&gt; Preferences\n\nUnder the General tab, select:\n\n“Save Workspace”\n“Use Keychain”\n“Info window always shows current selection” (Mac users: in the Browser tab)\nFor PC users: also select “double click opens file in external editor”\nSelect: “Editor,” select your text editor (Notepad++ is a good one for Windows users, if you want to download that), then check the box “Always use this application” to set it as the default.\n\nUnder the Bookmark tab, select:\n\n“New Bookmark”\nEnter a name for the bookmark in the box next to “Nickname:”\nEnter your password in the box next to “Password:”\nThen close out of the window (the little red X)\n\n\n\n\n\n\n\nOpen Connection (Click to enlarge)\n\n\n\n\n\n\nCredentials (Click to enlarge)\n\n\n\nYou can now launch Cyberduck and connect to the Cluster. If at any point you need to reconnect to the Cluster, you can go to “Bookmark” and select your bookmark.",
    "crumbs": [
      "Using the Cluster",
      "Connecting to the Cluster"
    ]
  },
  {
    "objectID": "chapter2-0.html",
    "href": "chapter2-0.html",
    "title": "16s Tutorial",
    "section": "",
    "text": "Summary TBD\nTutorial starts on next page (press next page button below of go to Programs & Data Aquisition in the table of contents to the left)",
    "crumbs": [
      "16s Tutorial"
    ]
  },
  {
    "objectID": "chapter2-3.html",
    "href": "chapter2-3.html",
    "title": "JC Qiime2 Pipeline",
    "section": "",
    "text": "This tutorial will follow a general cycle. You will first open a file from the QIIME2_pipeline folder, edit the file, submit the file, and then look at the results. If the submission script was successful, you will move on to the next file and continue the cycle. If it was not, then you will look at the log file in your current directory, determine what went wrong, and then start the cycle over with the same script.",
    "crumbs": [
      "16s Tutorial",
      "JC Qiime2 Pipeline"
    ]
  },
  {
    "objectID": "chapter2-3.html#import-raw-data-into-qiime2",
    "href": "chapter2-3.html#import-raw-data-into-qiime2",
    "title": "JC Qiime2 Pipeline",
    "section": "Import Raw Data into Qiime2",
    "text": "Import Raw Data into Qiime2\nEach account contains a file called BIN600 this you should always move into this folder when working on the cluster. Inside that folder you should find the metadata and raw data already imported to the Cluster account and it should be ready for you to start analysis!\nBefore you start a quick note on notepad editors. Most all of your computers will come with a notepad software that you can use with Cyberduck to edit the scripts to submit to the cluster. However for ease of use downloading a software called Notepad ++. This notepad software helps colorcode items in the script to make it easier to read. While this is not required it is highly recommended if you plan on using a notepad app to edit the scripts. If you prefer to use the nano function this is not required.\nDownload link: https://notepad-plus-plus.org/downloads/v8.5.3/\nBefore we can do anything with Qiime2, we need to get our data into a format it can use. That’s what this section is for.\n\n1.1_make-mapping.sh\nThe mapping file is a list of files with sample IDs for Qiime2 to import. So, we need to make one before analyzing our raw data.\n\n\nMove into your QIIME2_pipeline-updated folder\n\n\n\n\n\n\n\nWhat command did you use to move into the QIIME2_pipeline-updated folder?\n\n\n\n\nOpen your 1.1_make-mapping.sh script, either with Putty/Terminal, using nano 1.1_make-mapping.sh or, through Cyberduck, by right-clicking on it and choosing Edit With and then a text editor.\nSet the working directory to your main data folder (the folder containing the raw_data folder)\n\nworkdir = # your main data folder\n\n\n\n\n\n\nIn Putty, which command can you use to print your working directory and why might this be helpful when setting a working directory in a script?\n\n\n\n\nSubmit the script to the cluster\n\nsbatch 1.1_make-mapping.sh\n\n\n\n\n\n\nsbatch needs to be able to find the job file. So, the above command works if you’re in the QIIME2_pipeline folder but, you can submit from the main data folder too if you want, using sbatch QIIME2_pipeline/1_raw_reads_import.sh. Don’t forget the folder that you submit from will contain the .o log file.\n\n\n\n\nThis should make a mapping.csv file in the folder you specified for “workdir”. The mapping.csv file should contain three columns: sample-id, absolute-filepath, and direction.\n\n\n1.2_raw_reads_import.sh\nThis script uses the mapping file that you copied to import your data into Qiime2.\n\n\nMove into your QIIME2_pipeline folder\nOpen your 1.2_raw_reads_import.sh script either with Putty/Terminal, using nano 1.2_raw_reads_import.sh or through Cyberduck, by right-clicking on it and choosing Edit With and then a text editor.\n\n\n\n\n\n\n\nDo you prefer to use nano or a text-editor such as Notepad++ to edit scripts? If you have a preference, why do you have this preference?\n\n\n\n\nSet the working directory to your main data folder (the location of your mapping file)\n\n\n\n\n\n\n\nFor example, workdir=/home/stenley/bin-600/\n\n\n\n\n\n\n\n\n\nImportant: Your working directory is going to look different than this one because your user directory will be named differently and the other directories you have have different names than the directories in this path.\n\n\n\n\n#!/bin/bash\n\n#SBATCH --cpus-per-task=1\n#SBATCH -J qiime2_import\n#SBATCH --output=R-%x.%j.out\n#SBATCH --error=R-%x.%j.err\n#SBATCH --partition=batch-high,batch-medium,batch-low\n\n#=======================================================#\n#   Imports raw fastq reads into a QIIME2 artifact.\n#\n#   June 29, 2018\n#=======================================================#\n\n# Set your working directory to your main data directory\nworkdir = /home/thomase/bin-600/\n\n# Set the name of the Qiime2 environment to use\nqiime=qiime2-2022.11\n\ncd $workdir\n\nsource ~/.bashrc\nconda activate $qiime\n\n## before running script make a mapping file as described in the \n## tutorial. This is important because it points to the forward and \n## reverse read which will be imported.\n\n# Imports data based on mapping file\nqiime tools import \\\n  --type 'SampleData[PairedEndSequencesWithQuality]' \\\n  --input-path mapping.csv \\\n  --output-path paired-end-demux.qza \\\n  --input-format PairedEndFastqManifestPhred33\nFor the --type flag, we almost always use: SampleData[PairedEndSequencesWithQuality], but refer to documentation if you are working with something different. You know you have Paired End sequences if you have R1 and R2 files, and you know if you have quality information if the extension is either .fastq or .fastq.gz. To see all available types do:\nconda activate qiime2-2022.11\nor,\nqiime tools import --show-importable-types\n\n\n\n\n\n\nWhat outputs can you expect to see from script 1.2? What files were used as the input?\n\n\n\n\nSubmit the script to the cluster\n\nsbatch 1.2_raw_reads_import.sh\nsbatch needs to be able to find the job file. So, the above command works if you’re in the QIIME2_pipeline folder. But, you can submit from the main data folder too if you want, using sbatch QIIME2_pipeline/1_raw_reads_import.sh.\n\n\n\n\n\n\nDon’t forget the folder that you submit from will contain the .o log file\n\n\n\n\nCheck status of job with squeue or watch squeue\n\n\n\n\n\n\n\n\nThis script can take a while. A .o file with the job number will be created once it’s finished. If you use watch squeue, you can use CTRL + C to exit that screen.",
    "crumbs": [
      "16s Tutorial",
      "JC Qiime2 Pipeline"
    ]
  },
  {
    "objectID": "chapter2-3.html#quality-checking-and-filtering",
    "href": "chapter2-3.html#quality-checking-and-filtering",
    "title": "JC Qiime2 Pipeline",
    "section": "Quality Checking and Filtering",
    "text": "Quality Checking and Filtering\nBefore figuring out what bacterial taxa your sequences represent, you will first need to make sure that you are only submitting high quality sequences for analysis. Reads from the sequencing instrument can vary in quality, so it is important to ensure that the reads you are working with are of high quality to help guarantee good downstream results. The quality of the Illumina MiSeq platform is great; however, it is still prone to errors (less than 1%). We perform quality filtering to remove low quality sequences and to truncate sequences when they begin dropping below a specified quality score. Retention of this data could otherwise lead to erroneous conclusions from downstream analyses.\nSequences are received in the “fastq” format. This format includes both the sequences themselves, as well as the quality of each base pair in those sequences, as shown below:\n\nThe first line includes the unique sequence ID followed by the nucleotide sequence. The quality scores of each position are listed after the plus sign, where each character represents an ASCII-encoded quality score. These characters can also be used to calculate the quality of the sequence as a whole.\nTo look at what one of our .fastq text files looks like, choose one of the file names and use the zcat and head commands to view the first 10 lines in it. For instance, in the raw_data folder:\nzcat Chris_1C_S181_R1_001.fastq.gz | head\n\n2_quality_check.sh\nWe use this script to get quality information for the raw data.\n\n\nOpen the script using nano or Cyberduck\nSet the working directory to your main data folder. This script needs the paired-end-demux.qza file produced from the last one, and that should be in your main data folder. If you don’t see the paired-end-demux.qza file in Cyberduck, try hitting the refresh button towards the top of the window. You could also use ls to see if that file exists.\nBefore closing the script, note the flags and their values. In Qiime2, flags that begin with --i denote files used as input for the command. Likewise, --o flags denote flags specifying the names of output files. All filenames for viewable files generated by our pipeline begin with “VIEWABLE_”. So, if you decide to change the names of output files at any point during this tutorial or in the future, you will have to edit those flags and flags in subsequent scripts appropriately.\nSubmit the script to the cluster with sbatch 2_quality_check.sh. sbatch needs to be able to find the job file. So, the above command works if you’re in the QIIME2_pipeline folder.\nCheck status of job with squeue or watch squeue.\n\n\n\n\n\n\n\nThis script can take a while. .out and .err files with the job number will be created once it’s finished. If you use watch squeue, you can use CTRL + C to exit that screen.\n\n\n\n\n\n\n\n\n\nWhat output can we expect from this script?\n\n\n\n\nDownload the output .qzv files.\nGo to view.qiime2.org and then drag and drop the downloaded VIEWABLE files into the Drag and drop area (you have to do one at a time).\nIt’ll bring you to an Overview page (shown below), which will have information about the number of sequences per sample. If you scroll down further than I did, you’ll see the number of sequences for each sample.\n\n\n\nClick on the Interactive Quality Plot tab (shown in the gold rectangle above).\n\n\n\nThe x-axis shows the position of the base, and the y-axis shows the average Phred score at that position.\nA Phred quality score (more generally known as a “quality” or “Q” score) is a measure of accuracy for a base in a sequence; they indicate the probability that a base call is correct. For example, if a base has a Phred score of 20, the chance that this base call is correct is 99%. Phred scores are calculated with a logarithm, so a Phred score of 30 indicates that the probability of a correct base call is 99.9% for a certain position. The average Phred score of a sequence is sometimes used to evaluate its quality. Usually, an average above 30 is considered very good quality.\nReview the two text files that are created as well. Generally, I would recommend looking at the Hi_EE columns in each to see at what position it reaches 0.5 and 1.0 and then use the position immediately before it. Chances are those positions will be different between the R1 and R2 files. In script 3, you’ll see there’s different flags to specify them.\n\n\n\n\n\n\n\nUpdate 4/6/2023: Instead of those two text files, an additional visualization (VIEWABLE_ee_paired-end-demux.qzv) is created. The fastq_eestats tab has the information that was in the text files, with the Forward Reads table above the Reverse Reads table.\n\n\n\n\n\n\n\n\n\nBased on the view of this file how would you describe the overall quality of the sequence run?\n\n\n\n\n\n3_dada_denoise.sh\nWe will filter by length and expected error. Average expected error is the percentage of bases expected to be incorrect per 100 bases. So by using an expected error of 1, we are allowing one base of each 100 bases to be incorrect, for an expected error of 0.5, we are allowing one base of every 200 bases to be incorrect, and so on.\nTypically, an average expected error of 0.5 is considered very good sequence quality.\n\n\nAs with the other two scripts, open and edit it with nano or Cyberduck.\nSave and submit the script with sbatch\n\n\nHere are a list of flags for the qiime dada2 denoise-paired command:\n\n\n--i-demultiplexed-seqds: Qiime2 artifact containing paired end sequences with quality info\n--p-trim-left-f: Number of bases to trim at the beginning of the R1 reads\n--p-trim-left-r: Number of bases to trim at the beginning of the R2 reads\n--p-trunc-len-f: Position of last base to keep for the R1 reads\n--p-trunc-len-r: Position of last base to keep for the R2 reads\n--o-table: Name of Qiime2 artifact output table\n--o-representative-sequences: Name of Qiime2 artifact for the ASV’s representative sequences\n--o-denoising-stats: Name of Qiime2 artifact for the dada2 filtering stats\n--p-n-threads: Number of threads to use (should match ppn at the top of the script)\n--p-max-ee-f: Max expected error for the R1 reads (usually use 0.5 or 1.0)\n--p-max-ee-r: Max expected error for the R2 reads (usually use 0.5 or 1.0)\n\nA lot of the commands that we run in the tutorial up until alpha diversity essentially just have flags for input and output, and if they do have other flags, it’s not necessary to change them.\nStill, if you are ever curious about a Qiime2 command, you can see its flags by activating the Qiime2 environment (conda activate qiime2-2022.11) and then typing the command name followed by --help, e.g. qiime dada2 denoise-paired --help.\nI’ll also note that whenever you see --p-n-threads, it should match ppn at the top of the script (refer back to the Anatomy of a Script section if you have trouble finding that). The higher those are, the faster the script will run.\nHowever, our cluster doesn’t have infinite resources, so if you’re doing this in a workshop setting, please do not exceed 10 ppn.\n\n\n\n\n\n\nWhat files are created by this script? What files were used as the input?\n\n\n\nLet’s go through the VIEWABLE output from this script…\nNote: You’ll have two sets of these, and what I’ll be showing are screenshots from different data. All of these files are viewable by downloading them to your computer (See the “Using Cyberduck” section if you’re having trouble) and then dragging and dropping them onto the box at view.qiime2.org.\nGoing by the order of their creation, we have VIEWABLE_denoising-stats.qzv first.\n\nThe first column has the sample ids. The “input” column gives the number of raw sequences per sample, so this column tells you how many sequences you started with.\nAll the other columns give sequence counts (or percents) after each stage of the Dada2 filtering process. Besides the “input” column, the other ones you’ll want to note are the “non-chimeric” columns.\nThose columns give information about how much data were retained after all of the filtering steps. The numbers in the ‘non-chimeric’ column here are equal to the total number of Amplicon Sequence Variants (ASVs) in the sample, i.e. the total number of sequences retained. See here for more information about ASVs. But, for a brief description, they are basically OTUs clustered at 100% similarity, as all sequences assigned to the same ASV are identical. Anyway, the ‘percentage of input non-chimeric’ column has the amount of sequences retained as a percent of the total initial number.\nSo, looking at the above, the topmost sample (A1) had 14,090 raw sequences, and 9,151 remained after filtering, merging, and chimera removal. Those 8,452 sequences were all assigned to ASVs. So, 64.95% of the data were retained.\nGenerally, we want to retain at least half the sequences in nearly all of our samples. The CSO data has pretty poor quality, so if you’re using that data, since this is just a learning experience, the very large data loss is alright.\nNext, we’ll look at the VIEWABLE_unfiltered_table.qzv file.\n\nIt gives information about the number of ASVs per sample (remember this is equivalent to the values in the non-chimeric column for the previous visualization) and the frequency of ASVs.\nThe Overview tab provides summary statistics and histograms for both. The Table summary section tells you how many total samples you have, the number of unique ASVs, and the total number of ASVs in all the samples.\n\nThe Interactive Sample Detail tab gives the ASVs counts for each sample, with the samples ordered by decreasing frequency. One useful function here is the Sampling Depth Slider (shown in the dark blue rectangle).\nYou can see in the above picture that I have it set to a depth of 50,479. And, in the sentence below that, Qiime2 says how many and what percentage of samples and ASVs would be retained if I subsampled (rarefied) at that depth.\nSo, for the CSO data, at that depth, 302,874 ASVs and 6 samples would be retained. This would probably not be a good depth to use for these data because most of our samples would be excluded.\n\n\n\n\n\n\nWhat sampling depth do you think would be a good one to pick for your data? Why did you pick this?\n\n\n\n\nThe Feature Detail tab lists the ASVs by decreasing total Frequency. It also says how many samples each is found in.\nSpeaking of ASVs, the last visualization for this script for us to talk about is VIEWABLE_rep-seqs.qzv.\n\nOne representative sequence from each ASV is present in this file.\nBasic stats are given for these sequences. The Seven-Number Summary table shows the proportion of sequences that are that length or shorter, with the proportion in the header and the length in the row.\nThe sequences themselves are viewable in the table towards the bottom.",
    "crumbs": [
      "16s Tutorial",
      "JC Qiime2 Pipeline"
    ]
  },
  {
    "objectID": "chapter2-3.html#phylogenetic-and-taxonomic-information",
    "href": "chapter2-3.html#phylogenetic-and-taxonomic-information",
    "title": "JC Qiime2 Pipeline",
    "section": "Phylogenetic and Taxonomic Information",
    "text": "Phylogenetic and Taxonomic Information\nAt this point, we have our ASV table, but we don’t know which bacteria are actually in our samples or how similar the ASVs are to each other.\nThe scripts in this section address both of these issues.\n\n4_assign_tax.sh\nWe use a pre-trained classifier to determine which bacteria are present in our data.\nThis is the first script that can make use of a metadata file.\nIf you don’t have a metadata file at this point, you could either make one with just a column for sample ids, or you could just comment out this command and flags.\nIf you do have a metadata file, you may wish to validate it first with https://keemei.qiime2.org/. Keep in mind, Keemei will only tell you if there’s any formatting errors; it cannot tell you if the sample id’s in your data match the id’s in your metadata file.\n\n\nOpen the script and edit it with the nano command or Cyberduck\nSet the working directory to your main data folder. Check that the name of the .qza file with your representative sequences matches what’s after the --i-reads flag. If it doesn’t, change what’s after the flag so the names match. Make sure the value of --p-n-jobs matches ppn at the top of the script.\nSave and submit the script with sbatch\nOpen the .o log file associated with your job number and check that no errors occurred.\nCheck to see that the expected output files were created\nTake a look at the VIEWABLE_taxonomy.qzv file\nAs before, download it, then drag and drop it onto Qiime2’s website.\n\n\n\nYou’ll see a table with three columns.\n\n\nThe first column gives the names of the ASVs\nThe second gives the assigned taxonomy\nThe third reports how confident the classifier is that it’s right about the ASVs identities\nNext, let’s look at the VIEWABLE_taxa-bar-plots.qzv file\nAgain, download it, then drag and drop it onto Qiime2’s website.\n\n\n\nAs you can see, it creates a bar plot of the relative abundances, with each sample having a total sum of 100. If you mouse over one of the bar segments, the relative abundance will be shown for that bacteria in that sample, as seen with Betaproteobacteriales above.\nYou can change the taxonomic level with the drop down box under Taxonomic Level. In the above picture, orders are shown.\n\n\n\n\n\n\nWhat level would you filter the table by if you wanted to look at the species level?\n\n\n\nThe color palette is also editable, as is how the samples are sorted by. I chose to sort mine by the “Day” column in my metadata file so samples from the same day are next to each other, with gaps between samples from different days.\nBy default, the barplot is generated without referring to a metadata file, so you can sort by the abundance of a given taxa but not by any of your metadata groupings.\n\n\n\n\n\n\nWhat was the purpose of this script and how did it achieve what it was supposed to do?\n\n\n\n\n\n5_alpha_diversity.sh\nWe use this script to generate a phylogenetic tree, so this tree shows how closely related we think our ASVs are to each other.\nIf you’re curious about how this script works, see the “Generate a tree for phylogenetic diversity analyses” section here.\nBasically, the representative ASV sequences are aligned, in other words similarities between sequences are determined. Those alignments are then used as the basis for determining the phylogeny.\n\n\nOpen the script and edit it with the nano command or Cyberduck\nSet the working directory to your main data folder. Check that the name of the .qza file with your representative sequences matches what’s after the --i-reads flag. If it doesn’t, change what’s after the flag so the names match\nSave and submit the script with sbatch\n\n\n\n\n\n\n\n\nWhat are the viewable output files in this script if there are any?",
    "crumbs": [
      "16s Tutorial",
      "JC Qiime2 Pipeline"
    ]
  },
  {
    "objectID": "chapter2-3.html#downstream-analysis-preparation",
    "href": "chapter2-3.html#downstream-analysis-preparation",
    "title": "JC Qiime2 Pipeline",
    "section": "Downstream Analysis Preparation",
    "text": "Downstream Analysis Preparation\nThe 6.0 script can only be run when at least one negative control is present.\nThe 6.1 script doesn’t yield any new information, but it does put all the necessary files into folders for alpha and beta diversity analysis in addition to providing a few input folders for other types of analyses.\n\n6.0_decontam.sh\nOne of the greatest things about bacteria is they’re basically everywhere. One of the worst things about bacteria is, they’re basically everywhere.\nContamination is often a concern when analyzing bacterial genetic data. Contamination refers to bacteria that are in your data now but didn’t originate from your samples. So, these bacteria could have gotten in through vectors, such as the reagents used, the air, etc.\nBecause of the potential for contamination, negative controls are often sequenced along with samples.\n\n\n\n\n\n\nIf you don’t have negative controls or your negative controls had very few sequences (&lt;500), you can skip this script.\n\n\n\n\n\nOpen the script with nano or Cyberduck\nChange the working directory to your main data folder\nSpecify the name of your .qza table (input=)\nSpecify the name of the metadata file (meta=)\nSpecify the column in the metadata file that says whether a sample is a negative or not (column=)\nSpecify the value in that column for samples that are negatives (value=)\nSpecify the name of the final decontaminated .qza table (output=)\nSpecify the name of the table containing the list of ASVs filtered out as contaminants (contam=)\nSave and submit the script\n\nUse the output .qza table (decontam-table.qza by default) with script 6.1\n\n\n\n\n\n\n\n\n\nWhat is the purpose of running this script? Do you always use this script? When do you use it and when do you not?\n\n\n\n\n\nFiltering Guidelines\nThe next script (6.1) is used for filtering our data based on number of sequences, but before we do that, it’d be good to talk about some general guidelines for filtering.\nWe typically filter out samples that contain less than 1,000 sequences because it’s unlikely that the sequences present are accurate representations of the communities they originated from.\n\n\nView the VIEWABLE_unfiltered_table.qzv file again using view.qiime2.org\n\nIf script 6.0 was run, refer to the summarized-$output text file instead (where output is whatever you specified that variable as in script 6.0)\n\nClick on the Interactive Sample Detail tab (shown below in the orange rectangle)\n\n\n\nThis will show you all the samples and the number of sequences in each in descending order\nLook at this table and choose the minimum number of sequences you want your samples to have\n\n\n\n\n\n\n\n\nI would recommend filtering out samples that have less than 1,000 sequences\n\n\n\n\n\n\n\n\n\nHow many samples have less than 1000 sequences? What file did you look at to find this information?\n\n\n\n\n\nA Quick Note about Variables\n6.0 and 6.1 are set up a little differently compared to the previous scripts. Instead of specifying options next to flags in the commands themselves, you set the value of variables to what you want them to be and these variables are used in the commands.\n\n#!/bin/bash\n\n#PBS -l nodes=1:ppn10\n#PBS -N Q2_export_data\n#PBS -j oe\n\n#========================================================================#\n# Export ASV table and filter it\n# Makes alpha_div, beta_div, lefse, R, and exported-feature-table folders\n# Change names of table, taxonomy, and tree files to match yours\n#\n# Novemeber 5, 2019\n#========================================================================#\n\n# Set working directory to your main data directory \n# (folder with metadata, table, taxonomy, and tree files)\nworkdir = /home/USERNAME/16S_tutorial/data\n\ncd $workdir\n\n# Set sequence count cutoff set it to 0 if you don't want\n# to filter the table at all\ncutoff=1000\n\n# Specify name of files\nmeta=meta.txt\ntable=table.qza\ntaxonomy=taxonomy.qza\ntree=rooted-tree.qza\nYou can see in the above example: I’m filtering out all samples that have less than 1,000 sequences; my metadata file is named meta.txt; my table file is table.qza; my taxonomy file is taxonomy.qza, and lastly, my rooted tree file is rooted-tree.qza.\nThe section in the purple rectangle is creating variables with those values which are then used later in the script.\n\n\n\n\n\n\nThere are no spaces between the variable’s name, the equal sign, and the value.\n\n\n\nYou can see the benefits of using variables in the below picture…\n\n#Export tree\n\nqiime tools export \\\n  --input-path $tree \\\n  --output-path R\n\nmv R/tree.nwk R/rooted_tree.nwk #rename the file\n\ncp $tree alpha_div\nmv alpha_div/$tree alpha_div/rooted-tree.qza\n\ncp $tree beta_div\nmv beta_div/$tree beta_div/rooted-tree.qza\nThe tree variable is being used five times.\nChances are your tree file is also named rooted-tree.qza, but if it weren’t and we didn’t use variables in this script, you would have to specify the name of your tree file five times, instead of just once. So, using variables can save you a lot of time.\nYou’ll also notice that $tree is in orange. Depending on your text editor, it might not have any special formatting, but the dollar sign in front of “tree” is what tells the cluster to get the value of the tree variable instead of looking for a file literally named “tree.”\nAs an example, for me, cd $workdir is equivalent to cd /home/see/16S_tutorial/data. But, cd workdir would have the cluster look for a file actually named “workdir”.\nWe’ve actually been using variables this entire time. When you’ve specified the value for workdir, the workdir variable was then used to change to your working directory.\nPutting spaces in these lines is a very easy mistake to make, and from experience, I can say it’s pretty hard to tell you made it if you just glance through the script before submitting.\n\n\n\n\n\n\nIn the above picture which value is the variable?\n\n\n\n\n\nA Quick Note on Normalization\nNormalization basically refers to modifying the counts for our samples to make them more comparable to each other.\nConsider a situation in which one sample has 5,000 sequences (Sample A) and another 15,000 (Sample B). We may find that Sample B has a lot more species of bacteria than Sample A, but this could just be because Sample B had more data available, whether because PCR worked better for it or whatever other reason.\nOne way to make the samples more comparable would be to randomly select 4,000 sequences from A and 4,000 from B and then see if B still has more species than A. In fact, subsampling like this is a pretty common normalization strategy called rarefaction. And, we will use it for our alpha diversity analyses (don’t worry, if you don’t recognize that term, we’ll talk about it later).\nAnother common normalization strategy that we’ll be using is Cumulative Sum Scaling, or CSS normalization. The CSS technique corrects bias in the assessment of differential abundance of our data by dividing raw counts by the cumulative sum of counts up to a certain percentile determined by our data. We use data normalized with CSS for beta diversity analyses, while we use rarefactions to normalize data for alpha diversity.\n\n\n6.1_format_data.sh\nNow, that you have a bit more background information about filtering and variables, we are ready to start on script 6.1.\nIn addition to filtering out low abundance samples and Mitochondria and Chloroplasts, this script exports the rooted tree, table, and taxonomy .qza files.\nIt also creates folders for use with alpha and beta diversity analysis, biomarker analysis, and R.\n\n\nOpen the script with the nano command or Cyberduck\nChange the working directory to your main data folder\nChoose the minimum number of sequences a sample needs to be retained. If you want to keep all of your sequences, use 0\n\ncutoff=\n\nSpecify the names of your metadata, table, taxonomy, and tree files\n\nmeta=\ntable=\ntaxonomy=\ntree=\nIF YOU RAN DECONTAM (6.0), table SHOULD BE decontam-table.qza (without quotation marks)\n\nSave script and then submit it with sbatch\nCheck the .o and .err log files\nCheck that the alpha_div, ancom, beta_div, exported-feature-table, and R folders were created\n\nalpha_div folder should contain your metadata file, rooted-tree.qza file, filtered_feature-table.qza, and a VIEWABLE file for that table.\nancom folder should contain your metadata file and filtered_feature-table.qza.\nbeta_div folder should contain your metadata file, rooted-tree.qza file, and norm_feature-table.qza.\nR folder should contain your metadata file (like all the other folders), rooted_tree.nwk file, unnorm_R_table-with-taxonomy.txt file, and norm_R_table-with-taxonomy.txt\n\nYou can do a lot with R, like random forest analysis, partial squares discriminant analysis, etc. For starters though, I recommend looking into the Phyloseq package.\nYou can use R on the cluster, but it’s easier to make nice plots if you use it on your own laptop\n\nTo do that, you’ll want to download R and then R Studio\n\nAnalyses with R are, unfortunately, not included with this tutorial.\n\nWith that being said, feel free to contact JCS at chensjr16@juniata.edu if there’s any specific analyses you’re interested in doing with R.\n\n\nThe exported-feature-table folder should have taxonomy.tsv, feature-table.biom, VIEWABLE_chloroplast-mitochondria_filtered-$table.qzv, and summarized_feature-table.txt files\n\nThe VIEWABLE file contains the sequence counts per sample after mitochondria and chloroplasts are removed; the summarized_feature-table.txt contains the sequence counts only for the samples retained after filtering with the sequence number cutoff.\nThere should also be three additional biom files: norm_filtered_$cutoff.feature-table.biom, norm_table-with-taxonomy.biom, and unnorm_table-with-taxonomy.biom\n\nThe first file was imported for use with Qiime2’s beta diversity commands\nThe other two were exported to the R folder\n\n\n\n\n\n\n\n\n\n\n\nWhat directories did script 6.1 create and what folders do they all contain?\n\n\n\n\n\n\n\n\n\nBefore moving on, check that your alpha_div, beta_div, and ancom folders have the following files.",
    "crumbs": [
      "16s Tutorial",
      "JC Qiime2 Pipeline"
    ]
  },
  {
    "objectID": "chapter2-3.html#alpha-diversity-analysis",
    "href": "chapter2-3.html#alpha-diversity-analysis",
    "title": "JC Qiime2 Pipeline",
    "section": "Alpha Diversity Analysis",
    "text": "Alpha Diversity Analysis\nFirst of all, let’s talk about what alpha diversity actually is. We’ve mentioned it a couple times, but we haven’t actually defined it. Basically, alpha diversity just refers to within sample diversity.\nSo, imagine, we have two samples, A and B. A has 30 different ASVs and B has 70 different ASVs. We could say that B has higher alpha diversity, because it has more different ASVs, and indeed, this is actually a measure of alpha diversity, called “observed.”\nHowever, perhaps, A has 5,000 sequences, while B has 10,000, around twice as many. So, you can see then, at least for this metric, B might just have higher diversity because it has more sequences. So, we can use a normalization strategy to better compare these two samples. For instance, we could randomly pick 4,000 sequences from A and 4,000 from B. This is called rarefying the data. So, we could re-word the previous sentence as: we could rarefy A and B at a depth of 4,000 sequences.\nWe could then see how many different features are in those sub-samples. Now, of course, you probably wouldn’t get the same exact number if you were to repeat this process, so we repeat it a certain number of times (as defined by the --p-iterations flag) and end up averaging the results.\nWhat’s more, we do this at multiple depths, with the minimum being defined by --p-min-depth and the maximum by --p-max-depth. This allows us to see how alpha diversity, as defined by our different metrics, changes as we use more and more sequences, which is actually really important. We can plot the alpha diversity as the y axis and number of sequences as an x axis to show how that diversity changes with respect to depth. These plots are called rarefaction curves.\nOne common criticism of rarefaction as a normalization strategy is the fact that you’re not using a lot of your data. But, by sampling at multiple depths, we can see if the slope of the alpha diversity metrics levels off as we approach our maximum depth. If it does, then we probably used enough of our data to capture a good approximation of the actual alpha diversity present in our samples. However, if the slope is still rather steep, a higher maximum should be used.\nStill, this loss of data is the reason that we only use rarefaction for alpha diversity, instead of for both alpha and beta diversity analyses.\n\n\n\n\n\n\nIn your own words describe what Alpha diversity is Give an example.\n\n\n\n\n7.1_alpha_rarefactions.sh\nThis script rarefies our data at multiple depths, multiple times for each depth, and creates viewable rarefaction plots so we can see if we chose a sufficiently high depth.\nYou might want to look at the VIEWABLE_filtered_$cutoff.feature-table.qzv file or the summarized_feature-table.txt file in the alpha_div folder\nQiime2 needs a minimum and maximum depth. It will choose ten intermediary depths to sample at as well.\nYour maximum depth should be a little less than the smallest number of sequences possessed by your samples, at least the first time you run this script.\nBut, you may find that a higher depth is needed to accurately reflect your samples’ alpha diversity.\nYou can set your minimum depth to a tenth of that.\n\n\nChange your working directory to your alpha_div folder\nCheck that the value of --m-metadata-file matches the name of your metadata file\nChange the values for the other flags as desired\n\n\n\n--i-table: path to feature table\n--i-phylogeny: path to tree (for phylogenetic measures)\n--p-max-depth: maximum rarefaction depth\n--p-min-depth: minimum rarefaction depth\n--p-iterations: number of times to sample at each depth\n--m-metadata-file: name of metadata file\n--0-visualization: name of output file\n--p-metrics: metrics to calculate, each metric needs a separate flag on a separate line\n\n\nYou can see additional alpha diversity metrics here under the Alpha Diversity Analysis section.\nSave and submit the script\nReview the .o and .err log files to see if there were any errors\nOpen the resulting visualization file\n\nLook at the --o-visualization flag again if you’re having trouble finding it\nIt should have that name and be in your alpha_div folder if the script worked\n\n\n\n\nThese are rarefaction curves. The sequencing depth is the x axis, and the alpha diversity metric is the y axis.\nQiime2 lets you easily switch between the different metrics you used (by using the box under Metric) and the metadata column (by using the box under Sample Metadata Column).\n\n\n\nThis is what the legend should look like.\nIf you scroll down further, you’ll see a legend (see image to right) saying what categories the different colors correspond to.\n\n\n\n\n\n\nBased on the above image, which day had the lowest alpha diversity, and roughly, what is its value for the Observed Features alpha diversity metric?\n\n\n\n\n\n\n\n\n\nWhat value did you choose for the minimum and maximum depths for script 7.1? Describe what the graph looks like and why it looks that way.\n\n\n\n\n\n7.2_alpha_format.sh\nWhat we want to do now is average the values for the iterations. At each depth, however many iterations you specified are performed. Every iteration generates a table with alpha diversity values for each sample. So, if we did 20 iterations, each sample would have 20 values to average for each metric.\nBut after we do that, we need to get those averages into Qiime2. And, this script does that.\n\n\nOpen up the script\nChange the working directory to your alpha_div folder\nSpecify the maximum rarefaction depth (max= from last script)\n\nmax=\n\nCheck that the value of --input-path for the qiime tools export command matches the name of the visualization file for the alpha rarefactions plot\nSave and submit the script\nCheck the .o log file for errors and check the q2_alpha folder, within the alpha_div folder, for one .qza file for each metric\n\n\n\n\n7.3_alpha_div_significance.sh\nWe can see if alpha diversity varies by categorical groups. For instance, if we had an experiment in which some samples were treated with antibiotics and others weren’t, we would probably expect the control group to have higher alpha diversity than the treatment group. We can use this script to see if alpha diversity does differ based on our metadata if you have categorical metadata groups.\n\n\nChange the working directory to your alpha_div folder\nSpecify the name of your metadata file\n\nmeta=\nYou don’t have to change any of them, but here are the flags for the qiime diversity alpha-group-significance command\n\n\n\n\n--alpha-diversity: path alpha diversity results file\n--m-metadata-file: path to metadata file\n--o-Visualization: name of visualization containing boxplots and PERMANOVA results\n\n\nSave and submit the script\nCheck the log file for errors\nThe alpha_sig_results folder within your alpha_div folder should contain one VIEWABLE .qzv file for each alpha diversity metric\nLook at the VIEWABLE files\n\n\n\nOnly categorical columns can be used for this analysis. But, it’ll list any columns that were excluded at the top in a yellow rectangle, and it’ll say why they were excluded.\nBelow that will be boxplots. You can change the metadata category the visualization shows for the plots and the PERMANOVA results by using the box under “Column.”\nThe formatted_alpha folder (from 7.2) within the alpha_div folder contains the data used to make these boxplots if you want to remake them using ggplot2 in R.\nBeneath the boxplots are the results of the PERMANOVA tests. PERMANOVA test were performed for all groups together, shown in the Kruskal-Wallis (all groups) section, and for each set of two groups, shown in the Kruskal-Wallis (pairwise) section.\n\nThe p-value column is usually the one we pay the most attention to. A p-value is the probability that you would get your data just due to chance if there were no differences between the groups.\nGenerally, a p-value of 0.05 or lower is considered significant, as in, if the p-value is less than or equal to 0.05, that’s good evidence that there is a real difference as the odds of getting our data just due to chance is very low.\n\n\n\n\n\n\nChoose one of your alpha diversity measures and give a brief explanation of its meaning. Is it significant?\n\n\n\n\n\n7.4_alpha_div_correlation.sh\nWe can also see if alpha diversity varies with numerical metadata using this script if you have any numerical metadata.\n\n\nOpen the script\nChange the working directory to your alpha_div folder\nSpecify the name of your metadata file\n\nmeta=\nYou don’t have to change any of them, but here are the flags for the qiime diversity alpha-correlation command\n\n\n\n\n--i-alpha-diversity: path to alpha diversity results file\nーm-metadata-file: path to metadata file\n--p-method: correlation method to use (choose “spearman” or “pearson”)\n--o0-visualization: name of visualization containing boxplots and PERMANOVA results\n\n\nSave and submit the script\nCheck the .o log file for errors\nThe alpha_corr_results folder within your alpha_div folder should contain one VIEWABLE .qzv file for each alpha diversity metric\nLook at the VIEWABLE files\n\n\n\nThe output here is pretty similar to 7.3.\nThe excluded metadata columns are shown in the yellow rectangle at the top.\nInstead of a boxplot, the graphic is a dot plot, with the metadata column value as the x-axis and the alpha diversity metric value as the y-axis",
    "crumbs": [
      "16s Tutorial",
      "JC Qiime2 Pipeline"
    ]
  },
  {
    "objectID": "chapter2-3.html#beta-diversity",
    "href": "chapter2-3.html#beta-diversity",
    "title": "JC Qiime2 Pipeline",
    "section": "Beta Diversity",
    "text": "Beta Diversity\nBeta diversity is how different samples are from each other, so between sample diversity, rather than within sample diversity.\nAs an illustration, imagine that we have two 16S samples, and we wanted to see how different they were from each other. Well, we could make a number line for abundance of E. coli in them, and then we’d have two points on a number line. Well, what if we added another bacteria, say Pseudomonas aeruginosa.\nTheoretically, we could go through every bacteria present in both samples and see how different their abundances are between the samples.\nThe various metrics for beta diversity use more complicated formulas than that to determine how different samples are from each other.\nSee the metrics under Beta Diversity Analysis section here for all available metrics.\nWe typically use the weight unnormalized unifrac metric, which takes into account phylogeny and abundance.\n\nA Quick Note on Distance Matrices\nWe make and then use a distance matrix for our beta diversity analyses, based on the CSS normalized table.\nA distance matrix shows how different, so how distant, samples are from each other.\n\n\nSample distance matrix.\n\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\nA\n0\n10\n12\n\n\nB\n10\n0\n2\n\n\nC\n12\n2\n0\n\n\n\nIn this distance matrix, B and C are more similar to each other than either is to A. A is more different from C (12) than it is from B (10).\nWe can use a distance matrix to create plots, like PCoA and NMDS, to visualize how similar samples are to each other.\nThese visualizations are called ordinations. They try to show the distances between samples as accurately as possible using only 2 or 3 dimensions.\n\n\n\n\n\n\nGive a brief description of Beta diversity Provide an example. Compare and contrast Alpha and beta diversity. How are they similar and how are they different?\nBased on the Image below what are the alpha and beta diversity measures.\n\n\n\n\n\n\n8.1_beta_input.sh\nThe statistical tests for beta diversity use a distance matrix to see if beta diversity varies with categorical and numerical metadata. This script calculates the distance matrix and creates PCoA coordinates based on them.\n\n\nOpen the script\nChange the working directory to your beta_div folder\nSave and submit the script\n\nYou don’t have to change anything else in the script but here are the flags, in case you want to change anything\nqiime diversity beta-phylogenetic or qiime diversity beta (if you use a non-phylogenetic metric)\n\n\n\n\n--i-table: path to feature table\n--i-phylogeny: path to tree (for qiime diversity beta-phylogenetic)\n--p-metric: metric to use\n--o-distance-matrix: name of resulting distance matrix\n\n\nqiime diversity pcoa\n\n\n\n--i-distance-matrix: path to distance matrix\n--p-number-of-dimensions: number of dimensions to use\n--o-pcoa: name of artifact file containing PCoA coordinates\n\n\nCheck that the output files have been created\n\nWhere will the output files be located (assuming the script worked)? Also, did JCS make a typo in the workdir line or was he just testing you? Note: if you say he made a typo, he will fail you.\n\n\n\n8.2_beta_div_significance.sh\nLike 7.3, you can only use this script if you have categorical metadata.\nThis script will run PERMANOVA for all groups in a category overall and pairwise. These tests allow you to see if samples in the same group are significantly more different from samples in other groups than they are from each other.\n\n\nSet the working directory to where the output from the previous script was created\n\nIt should be [VARIABLE]=[DIRECTORY]\n\nChange the name of the metadata file to match yours\n\nmeta=\n\nList the categories that you want to use for the PERMANOVA tests in the columns variable\n\n\n\n# Specify columns to test by, new line for each column\n# Pairwise PERMANOVA will be performed for each of these categories\ncolumns=(\n  Classification\n  Grade\n  Gender\n)\n\n\nEach category should be on a new line, and all of them should be within the parentheses\nFor the purposes of this tutorial, you can just use the Location and Site columns You do NOT need to use a third categorical column\n\n\nSave and submit the script\nLook at the output\n\n\n\nThis is a PCoA plot. Samples that are closer to each other are more similar. Each axis captures a certain amount of the variation among the samples. The first axis is always the most important, followed by the second, and then the third.\nYou can color samples by a metadata category. The plot above is colored by the categorical column “Day”. The colors used are from the Classic QIIME Colors palette. You can change the colors for each group by clicking on the colored box next to the category it denotes.\n\nLooking at the box in green, the topmost box “scatter:” is used to specify which metadata column to use to color the points. The next box is the colors. Here, we use the Red-Purple gradient. In the blue box, check the “Continuous values” box as shown for the colors to be used as a gradient, instead of having a different color for each value.\nNext, we’ll look at the results of the PERMANOVA tests\n\nThe topmost section gives the results of the PERMANOVA test that considered if samples in all of the groups were more similar to each other than samples in any of the other groups.\n\nThe middle part shows boxplots of the distances for samples in each group from the group given in the title. Based on this plot then, samples in the Day_5 group seem to be the most different from the Day_1 group. If you look back at the first PCoA picture, you’ll see the one Day_1 sample (red) is pretty far from the Day 5 samples (purple), so this result makes sense.\n\nThe lowest part of this visualization shows the results of the pairwise PERMANOVA tests.\n\n\n\n\n\n\nIf we use a cutoff of 0.05 for significance (α=0.05), how many of the pairwise comparisons above are significant?\n\n\n\n\n\n8.3_beta_div_correlation.sh\nAs with alpha diversity, beta diversity can also be correlated with numerical metadata. And, this script is used to do that.\n\n\nCopy the scripts with: cp -R /home/see/tutorial_data/8.3_scripts\nCopy the metadata with: cp /home/see/tutorial_data/metadata/*.txt\nOpen the script\nChange the working directory\nSpecify the name of your metadata file (leave it as is for these scripts)\nLeave the columns as is\nSave and submit the script\n\nAgain, here’s a list of the flags for the commands.\n\nqiime emperor plot \n\n\n--i-pcoa: path to artifact file with PCoA coordinates\n--i-distance-matrix: path to distance matrix\n--m-metadata-file: path to metadata file\n--o-visualization: name of visualization containing boxplots and PERMANOVA results\n\n\nqiime diversity beta-correlation \n\n\n--i-pcoa: path to artifact file with PCoA coordinates\n--i-distance-matrix: path to distance matrix\n--m-metadata-file: path to metadata file\n--o-visualization: name of visualization containing boxplots and PERMANOVA results\n--p-method: method to use for correlations (“spearman” or “pearson”)\n--o-metadata-distance-matrix: name of metadata distance matrix\n--o-mantel-scatter-visualization: name of visualization containing scatterplot and the stats test results\n--p-label1: label for axis plotting the values of the bacterial distance matrix\n--p-label2: Iabel for the axis plotting the values of the metadata distance matrix\n\n\nView the outputs\n\n\nThe metadata file has a lot of missing values, so that’s why we’re using five of those in this case. The commands we use for these scripts can’t tolerate missing values, evidently.\nThe timepoint script is a bit different. We can force timepoint to be an axis since we’re representing it with numbers, but it’s actually categorical, not continuous, e.g. there is no 3.14 timepoint, nor could there have been. So, we use the stats command from 8.2 for that one.\nAnyway, let’s look at the PCoA visualization\n\npH is now used as the first axis, and what was the first axis is now plotted where the second axis was.\nYou can use this to see if beta diversity is positively or negatively correlated with metadata. If it’s negatively correlated, we would expect samples to be closer together at higher values of the numerical metadata.\nNext, we’ll look at the results of the correlation command for the different columns.\n\nAs before, the yellow box lists sample id’s that were excluded.\nThe results of the statistical test are shown. The p-value is very high (0.719), so according to the Mantel test, there is a high probability that there is no correlation between pH and the distances between samples based on their bacterial composition.\nSpearman rho is the strength of the correlation, ranging from -1 to 1.\nThe last type of visualization for this script is for the results of the bioenv command\n\nBio-Env correlates multiple numerical metadata columns with the distances based on the bacterial community. In the above picture, you can see the strongest correlation is achieved by a combination of CONDUCTIVITY, TEMP, SALINITY, and TDS.",
    "crumbs": [
      "16s Tutorial",
      "JC Qiime2 Pipeline"
    ]
  },
  {
    "objectID": "chapter2-3.html#differential-abundance-analysis",
    "href": "chapter2-3.html#differential-abundance-analysis",
    "title": "JC Qiime2 Pipeline",
    "section": "Differential Abundance Analysis",
    "text": "Differential Abundance Analysis\nIn addition to knowing if alpha and beta diversity differ according to categorical metadata, it would also be nice to know if any bacteria are more abundant in certain categories compared to others.\nThere are a variety of programs and tests to determine that. But, for the purpose of this tutorial we will use ANCOM.\n\n9_diff_abundance.sh\nFirst off, you need to have metadata for all samples in the table; if you don’t, those samples need to be removed before running ANCOM. ANCOM assumes less than 25% of the features differ between groups. If you expect more than 25% are different, don’t use ANCOM.\nIt also can’t tolerate frequencies of 0, but we can use another Qiime2 command (qiime composition add-pseudocount) to account for that limitation.\nWe also run this on a collapsed table, using the qiime taxa collapse command, to make sure all ASVs used on this analysis are identified to the same level.\n\n\nOpen your metadata file in the ancom folder with Cyberduck\nCheck the column(s) that you want to use\n\nIf any samples in the table have missing values, they will need to be removed before running ANCOM\nWe use the qiime feature-table filter-samples command in this script to keep only the samples present in the metadata\nFor the purposes of this tutorial, you can just use the Location column\n\nDelete the rows for any samples with missing data for the column(s) that you’ll be using\n\n\n\nSave the metadata file\nChange the working directory to your ancom folder\nSpecify the name of your metadata file\nLike the beta diversity scripts, specify the columns you want to use with ANCOM\nSpecify the level you want to collapse at\n\nlevel=\nWe usually use 7 for species level analyses\n\nCheck that the values for the qiime taxa collapse command’s --i-table and --i-taxonomy flags match your file names\nSave and submit the script\n\n\nqiime taxa collapse\n\n\n\n--i-table: path to feature table\n--i-taxonomy: path to taxonomy file\n--p-level: level to collapse at\n--o-collapsed-table: name of collapsed table\n\n\nqiime feature-table filter-samples\n\n\n\n--i-table: path to feature table\n--m-metadata-file: path to metadata file\n--o-filtered-table: name of filtered table\n\n\nqiime composition add-pseudocount\n\n\n\n--i-table: path to feature table\n--o-composition-table: name of composition table\n\n\nqiime composition ancom\n\n\n\n--i-table: path to table to use\n--m-metadata-file: path to metadata file\n--m-metadata-column: name of categorical metadata column to use\n--o-visualization: name of visualization containing volcano plot\n\n\nLook at the resulting visualization files\n\n\n\n\n\n\n\n\n\nBased on your volcano plot generated how many different taxa are differenitally abundant. What does differentially abundant mean? Describe in your own words.\n\n\n\nLet’s break this down a little bit. W is the number of sub-hypotheses that were rejected. Each sub-hypothesis is testing whether the ratio of one bacteria, let’s say E. coli, to another bacteria, let’s say P. aeruginosa, is the same in one group as it is in another. So, the setup would look something like this,\n\\(H_0(_{E.\\ coli/P. aeruginosa}) = \\text{mean}\\left(\\log\\left(\\frac{\\text{Day1}_{E.\\ coli}}{\\text{Day1}_{P.\\ aeruginosa}}\\right)\\right) = \\text{mean}\\left(\\log\\left(\\frac{\\text{Day2}_{E.\\ coli}}{\\text{Day2}_{P.\\ aeruginosa}}\\right)\\right)\\)\nSo, the value of W for the ASV representing E. coli would be the number of times the null hypothesis is rejected for that ASV. In other words, the number of times the ratio of that ASV to another differs between groups.\nThe null hypotheses for the two ASVs shown in the table were rejected 2982 for both.\nThe clr value is the difference between the log abundance between that ASV (point) and the average log abundance of all ASVs.\nThe lower part of the visualization shows the abundances of the enriched ASVs (the ASVs in the table above) for each group.\n\nIt shows the abundances for each quartile in each group. However, in this example, Day_1 only had one sample (you would probably not want to use a group with only 1 sample in actual analyses), so that’s why all the values are the same for each percentile. These ASVs were probably not found in the other groups, so the pseudo-count command set their abundances to 1, which is why the percentile values for those are all 1.\nBioBakery’s LEfSe and the Aldex2 Qiime2 plugin are alternative methods for determining differential abundance.",
    "crumbs": [
      "16s Tutorial",
      "JC Qiime2 Pipeline"
    ]
  },
  {
    "objectID": "chapter2-3.html#lefse---biomarker-analysis",
    "href": "chapter2-3.html#lefse---biomarker-analysis",
    "title": "JC Qiime2 Pipeline",
    "section": "LEfSe - Biomarker Analysis",
    "text": "LEfSe - Biomarker Analysis\nPreviously, we used ANCOM to determine which bacteria were differentially abundant. LEfSe is another tool to do that.\n\n\n\n\n\n\nWhy You Would Use It: You want to see which, if any, bacteria differ significantly in abundance among categorical metadata groups.\n\n\n\n\nLEfSe Installation\n\nCreate LEfSe environment\n\nAlternatively: conda create --name lefse --file /home/see/YML_Files/lefsepkgs.txt\n\nCreate input formatting environment\n\nconda create -n tidyverse -y -c conda-forge r-tidyverse r-optparse r-funrar r-plyr\n\n\nLEFSe scripts are located in: QIIME2_pipeline/other_scripts/lefse_scripts\n\n\n1_LEfSe_generic_summarize_taxa.sh\nThis script lets you choose the taxonomic level you want to use for LEfSe analyses; we usually use 7 for species. It uses that level to create formatted input files for the next script.\n\n\nOpen script\nSet the working directory to your ancom folder\n\nworkdir=\n\nSpecify metadata file\n\nmeta=\n\nSpecify categorical metadata columns that you want to use for LEfSe\n\ncolumns=\n\nSpecify level to summarize at, e.g. 7 for species, 6 for genus, etc.\n\nlevel=\n\nSpecify name of table to use\n\ntable=\n\nSpecify name of taxonomy file to use\n\ntaxonomy=\n\nSpecify column header for metadata column containing sample id’s\n\nid=\n\nSave and submit the script\nCheck that a lefse_ text file was created for each column\n\n\nThere should also be a LEFSE_intermediate_files folder that contains three files (one .biom file and two .txt’s that were generated during the course of running the script). These files are no longer needed, but they are there if you want to look at them.\n\n\n2_LEfSe_run.sh\nThis script performs the LEfSe analysis and creates output visualizations, in png form. The x-axis for the bar plot visualization will be the log(LDA) score, which is a measure of enrichment.\nImportantly, this script also performs CPM normalization on the data. That’s basically the same as relative abundance, except all the counts in a sample sum to 1,000,000, instead of 1 or 100.\nLEfSe performs Kruskal-Wallis tests to determine significant differences in abundance based on the metadata group and then uses Linear Discriminant Analysis to get a measure of how much its abundance differs.\n\n\nOpen script\nSet the working directory to your ancom folder\n\nworkdir=\n\nSpecify the minimum LDA score for a taxon to be considered enriched\n\nlda=\nWe usually use a cutoff of 2\n\nChange the values of the other variables as desired\nSave and submit the script\nCheck that the initial_figures and results_res folders contain two files per sample\n\nThe initial_figures folder should have a cladogram and a bar plot for each comparison\n\n\n\nThis is an example bar plot from LEfSe. The colors indicate the group that is enriched in, and the x-axis indicates the strength of that enrichment.\n\nHere’s a cladogram. It doesn’t show the LDA scores, but it shows the phylogenetic relationships among the enriched taxa.\n\nThe results_res folder contains the files that these figures are based on.\nFor each sample, it’ll have a .res file at the level you specified for the cutoff and another with a cutoff of 0.\n\n\nYou may have noticed that some of the names are missing in the above figures. That’s because those bacteria were not identified to species level, or even less specific levels in some cases.\nBut, you can change the names in the .res file using Excel and then use the edited file to re-make the figures.\nI recommend deleting all unenriched taxa to make the cladogram look better.\n\n\nSelect all the data\nGo to Sort & Filter (in the Home bar towards the far right)\nSort by Column C to bring all the enriched features to the top of the table\nDelete everything below the enriched features (all the rows without values in Column C)\n\n\nYou can change the names of the features (taxa) too by altering the cells in Column A. Periods separate the different taxonomic levels. The bar plot and cladograms both just show the last (most specific) one.\nSave altered .res file(s), with the prefix ed_ (no quotation marks in the actual name).\n\n\n3_LEfSe_redo_figures.sh\nThis script uses the edited (ed_*) .res files to remake the figures.\n\n\nMake sure your edited .res files begin with ed_\nOpen the script\nSet the working directory to the ancom folder (or wherever you have the edited files)\nChange the values of the variables as you desire\nSave and submit the script\nCheck that the edited_figures folder has a bar plot and cladogram for each .res file\n\n\n\nYou can see that I changed the unidentified taxa to their most specific taxonomic level, with Qiime2’s level prefix at the start and an abbreviation for the traditional taxonomic levels at the end, e.g. sp for species and g for genus.\n\n\n\n\n\n\nDo you think LEfSe would work with a column for patient sex that used “1” to represent female and “2” to represent male?\n\n\n\nIf you want to recreate the LEfSe figures with different colors, you can copy the script at: /home/see/lefse_scripts/4_LEfSe_colors.sh\nChange the value of the d1 variable to match the name of your .res file, and the values after the colors flag to match the hex codes of the colors you want to use.",
    "crumbs": [
      "16s Tutorial",
      "JC Qiime2 Pipeline"
    ]
  },
  {
    "objectID": "chapter2-2.html",
    "href": "chapter2-2.html",
    "title": "Qiime2 Basics",
    "section": "",
    "text": "QIIME2, with Qiime pronounced as “chime”, or Quantitative Insights Into Microbial Ecology 2, is an open source software package for comparison and analysis of microbial communities. Culture independent analysis of microbial communities has been enabled by high-throughput genetic sequencing and high-throughput data analysis. Multiplexing and high-throughput sequencing technologies, such as Illumina sequencing, allow scientists to perform parallel sequencing of hundreds of samples at high depths. Open-source bioinformatics pipelines, such as QIIME2, allow for robust analysis of millions of sequences. While these tools are powerful and flexible, they are complex and can be difficult to learn.\nThe core goal of this workshop is to create a standard pipeline that is accessible to first time QIIME2 users. QIIME2 is extremely flexible and can accommodate various sequencing technologies and methods of data analysis. This tutorial presents a set of scripts that will allow a user to quickly progress through “typical” analysis of 16S rRNA gene data. These scripts have been built for use with Illumina sequencing technology and the Juniata College HHMI computational environment.\nAll of the files that you will use with Qiime2 are either .qza (artifacts) or .qzv (visualizations). Qiime2 tracks how those files were created, which helps with reproducibility. The website view.qiime2.org is used to see this information and visualizations.\nArtifacts have different semantic types. These semantic types are basically analogous to different file types on your computer and help prevent users from applying analyses incorrectly.\nQiime2’s functionality is based on plugins. All the plugins that you need for the main pipeline are installed by default. However, you might find some plugins listed here interesting.\nThe Qiime2 website has a variety of tutorials if you want any additional practice or just want to see what else it can do. The Moving Pictures tutorial in particular has a lot of similarities with our pipeline.",
    "crumbs": [
      "16s Tutorial",
      "Qiime2 Basics"
    ]
  },
  {
    "objectID": "chapter2-2.html#installing-qiime2",
    "href": "chapter2-2.html#installing-qiime2",
    "title": "Qiime2 Basics",
    "section": "Installing Qiime2",
    "text": "Installing Qiime2\nWe will be installing Qiime2 by creating a miniconda environment, then installing Qiime2 into that environment. You can do this by running the following commands:\nwget https://data.qiime2.org/distro/core/qiime2-2022.11-py38-linux-conda.yml\nconda env create -n qiime2-2022.11 --file qiime2-2022.11-py38-linux-conda.yml\n\n\n\n\n\n\nThis may take a while to run.\n\n\n\n\n\n\n\n\n\nIf you get a HTTP error, try running the command again.",
    "crumbs": [
      "16s Tutorial",
      "Qiime2 Basics"
    ]
  },
  {
    "objectID": "chapter2-2.html#other-necessary-installations",
    "href": "chapter2-2.html#other-necessary-installations",
    "title": "Qiime2 Basics",
    "section": "Other Necessary Installations",
    "text": "Other Necessary Installations\nWe also need to create another environment; we’ll use this environment with script 6.1 and 7.2:\nconda env create -n biom-convert --file /home/see/YML_Files/biom-convert5.yml\nAnd, one more for script 6.0…\nconda env create -n decontam --file /home/see/YML_Files/decontam2.yaml\nThese environments can be activated using conda activate [NAME OF ENVIRONMENT]. So, for example:\nconda activate qiime2-2022.11\nAnd, they can be deactivated using the command:\nconda deactivate\nYou can check that your environments installed correctly by activating them and running a command that should now work with the --help flag and seeing if that brings up the command’s options.\nCheck that the biom-convert environment installed correctly by doing:\nconda activate biom-convert\nAlternatively to check, use:\nconda env list\n\n\n\n\n\n\nConfirm that your environments work. Use the command biom convert --help for the biom-convert environment and the command qiime tools import --help for the qiime2-2022.11 environment.",
    "crumbs": [
      "16s Tutorial",
      "Qiime2 Basics"
    ]
  },
  {
    "objectID": "chapter2-2.html#qiime2-files",
    "href": "chapter2-2.html#qiime2-files",
    "title": "Qiime2 Basics",
    "section": "Qiime2 Files",
    "text": "Qiime2 Files\nQiime2 uses .qza and .qzv files. Qiime2 .qza files are referred to as artifacts. These files contain data used in analyses, in addition to information about how that data were generated (provenance) and the file type, e.g. FeatureTable[Frequency]. Artifacts can be viewed at view.qiime2.org.\nYou can see all of the formats and types by using flags with the qiime tools import command after activating the environment:\nqiime tools import --show-importable-types\nqiime tools import --show-importable-formats\n\nThe details tab (shown above) gives the file name, the Qiime2 id (uuid), the type, and format. It also lists citations for the tools used to create the file below. The provenance tab (the tab in the blue rectangle above) contains information about how the data were generated.\n\nIf you click on the circle, that will just show you the uuid, type, and format again. But, if you click on the square surrounding it…\n\nYou’ll get basically all the information you could ever want about how it was made. As you can see above, it tells you when the file was made, how long its creation took, what plugin was used to make it, the inputs used, and even the parameters used for the command. This is great for keeping track of how Qiime2 data and figures were generated.\nLike artifacts, Qiime2 visualizations (.qzv files) also have Details and Provenance tabs when viewed with view.qiime2.org.\n\nBut, it has another tab, the Visualization tab. And, above, is the same data, except one additional command was used to convert the table from an artifact to a visualization, making it more readable and providing you with additional information about it.\n\nYou can see that there’s another square now in the provenance tab. That’s because of the additional command converting the artifact to a visualization, and on the right side, you can see that the details about that command are provided.\nBut, let’s look at the two sets of tabs. The tabs in the purple rectangle are the same for every visualization and artifact. In other words, all visualizations have Visualization, Details, and Provenance tabs, while all artifacts have Details and Provenance tabs.\nThe tabs in the orange box vary based on which tab in the purple area you’re in, and there may not even be any tabs in this area, as is the case when on the Details tab for the table.qza artifact above. For the Visualization tab, the tabs in the orange rectangle area also vary depending on the data being visualized and type of visualization.",
    "crumbs": [
      "16s Tutorial",
      "Qiime2 Basics"
    ]
  }
]